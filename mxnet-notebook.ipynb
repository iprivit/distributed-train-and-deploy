{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Using SageMaker Image Classification with Amazon Elastic Inference\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "  1. [Permissions and environment variables](#Permissions-and-environment-variables)\n",
    "3. [Training the ResNet model](#Training-the-ResNet-model)\n",
    "4. [Deploy The Model](#Deploy-the-model)\n",
    "  1. [Create model](#Create-model)\n",
    "  3. [Real-time inference](#Real-time-inference)\n",
    "    1. [Create endpoint configuration](#Create-endpoint-configuration) \n",
    "    2. [Create endpoint](#Create-endpoint) \n",
    "    3. [Perform inference](#Perform-inference) \n",
    "    4. [Clean up](#Clean-up)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to enable and use Amazon Elastic Inference (EI) for real-time inference with SageMaker Image Classification algorithm.\n",
    "\n",
    "Amazon Elastic Inference (EI) is a service that provides cost-efficient hardware acceleration meant for inferences in AWS. For more information please visit: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n",
    "\n",
    "This notebook is an adaption of the SageMaker Image Classification's [end-to-end notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-fulltraining-highlevel.ipynb), with modifications showing the changes needed to use EI for real-time inference with SageMaker Image Classification algorithm.\n",
    "\n",
    "In this demo, we will use the Amazon SageMaker image classification algorithm to train on the [caltech-256 dataset](http://www.vision.caltech.edu/Image_Datasets/Caltech256/). \n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication to AWS services. There are three parts to this:\n",
    "\n",
    "* The roles used to give learning and hosting access to your data. This will automatically be obtained from the role used to start the notebook\n",
    "* The S3 bucket that you want to use for training and model data\n",
    "* The Amazon SageMaker Image Classification docker image which need not be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket=sess.default_bucket()\n",
    "prefix = 'ic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Download the data and transfer to S3 for use in training. In this demo, we are using [Caltech-256](http://www.vision.caltech.edu/Image_Datasets/Caltech256/) dataset, which contains 30608 images of 256 objects. For the training and validation data, we follow the splitting scheme in this MXNet [example](https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/data/caltech256.sh). In particular, it randomly selects 60 images per class for training, and uses the remaining data for validation. The algorithm takes `RecordIO` file as input. The user can also provide the image files as input, which will be converted into `RecordIO` format using MXNet's [im2rec](https://mxnet.incubator.apache.org/how_to/recordio.html?highlight=im2rec) tool. It takes around 50 seconds to convert the entire Caltech-256 dataset (~1.2GB) on a p2.xlarge instance. However, for this demo, we will use record io format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import urllib.request\n",
    "import boto3\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "        \n",
    "def upload_to_s3(channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "\n",
    "# caltech-256 (RecordIO format)\n",
    "#download('http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec')\n",
    "#download('http://data.mxnet.io/data/caltech-256/caltech-256-60-val.rec')\n",
    "\n",
    "# Caltech-256 image files\n",
    "download('https://caltech256-bucket.s3.amazonaws.com/256_ObjectCategories.tar ')\n",
    "# !tar -xf 256_ObjectCategories.tar\n",
    "\n",
    "# Tool for creating lst file\n",
    "download('https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001.ak47 0\n",
      "002.american-flag 1\n",
      "003.backpack 2\n",
      "004.baseball-bat 3\n",
      "005.baseball-glove 4\n",
      "006.basketball-hoop 5\n",
      "007.bat 6\n",
      "008.bathtub 7\n",
      "009.bear 8\n",
      "010.beer-mug 9\n",
      "011.billiards 10\n",
      "012.binoculars 11\n",
      "013.birdbath 12\n",
      "014.blimp 13\n",
      "015.bonsai-101 14\n",
      "016.boom-box 15\n",
      "017.bowling-ball 16\n",
      "018.bowling-pin 17\n",
      "019.boxing-glove 18\n",
      "020.brain-101 19\n",
      "021.breadmaker 20\n",
      "022.buddha-101 21\n",
      "023.bulldozer 22\n",
      "024.butterfly 23\n",
      "025.cactus 24\n",
      "026.cake 25\n",
      "027.calculator 26\n",
      "028.camel 27\n",
      "029.cannon 28\n",
      "030.canoe 29\n",
      "031.car-tire 30\n",
      "032.cartman 31\n",
      "033.cd 32\n",
      "034.centipede 33\n",
      "035.cereal-box 34\n",
      "036.chandelier-101 35\n",
      "037.chess-board 36\n",
      "038.chimp 37\n",
      "039.chopsticks 38\n",
      "040.cockroach 39\n",
      "041.coffee-mug 40\n",
      "042.coffin 41\n",
      "043.coin 42\n",
      "044.comet 43\n",
      "045.computer-keyboard 44\n",
      "046.computer-monitor 45\n",
      "047.computer-mouse 46\n",
      "048.conch 47\n",
      "049.cormorant 48\n",
      "050.covered-wagon 49\n",
      "051.cowboy-hat 50\n",
      "052.crab-101 51\n",
      "053.desk-globe 52\n",
      "054.diamond-ring 53\n",
      "055.dice 54\n",
      "056.dog 55\n",
      "057.dolphin-101 56\n",
      "058.doorknob 57\n",
      "059.drinking-straw 58\n",
      "060.duck 59\n",
      "061.dumb-bell 60\n",
      "062.eiffel-tower 61\n",
      "063.electric-guitar-101 62\n",
      "064.elephant-101 63\n",
      "065.elk 64\n",
      "066.ewer-101 65\n",
      "067.eyeglasses 66\n",
      "068.fern 67\n",
      "069.fighter-jet 68\n",
      "070.fire-extinguisher 69\n",
      "071.fire-hydrant 70\n",
      "072.fire-truck 71\n",
      "073.fireworks 72\n",
      "074.flashlight 73\n",
      "075.floppy-disk 74\n",
      "076.football-helmet 75\n",
      "077.french-horn 76\n",
      "078.fried-egg 77\n",
      "079.frisbee 78\n",
      "080.frog 79\n",
      "081.frying-pan 80\n",
      "082.galaxy 81\n",
      "083.gas-pump 82\n",
      "084.giraffe 83\n",
      "085.goat 84\n",
      "086.golden-gate-bridge 85\n",
      "087.goldfish 86\n",
      "088.golf-ball 87\n",
      "089.goose 88\n",
      "090.gorilla 89\n",
      "091.grand-piano-101 90\n",
      "092.grapes 91\n",
      "093.grasshopper 92\n",
      "094.guitar-pick 93\n",
      "095.hamburger 94\n",
      "096.hammock 95\n",
      "097.harmonica 96\n",
      "098.harp 97\n",
      "099.harpsichord 98\n",
      "100.hawksbill-101 99\n",
      "101.head-phones 100\n",
      "102.helicopter-101 101\n",
      "103.hibiscus 102\n",
      "104.homer-simpson 103\n",
      "105.horse 104\n",
      "106.horseshoe-crab 105\n",
      "107.hot-air-balloon 106\n",
      "108.hot-dog 107\n",
      "109.hot-tub 108\n",
      "110.hourglass 109\n",
      "111.house-fly 110\n",
      "112.human-skeleton 111\n",
      "113.hummingbird 112\n",
      "114.ibis-101 113\n",
      "115.ice-cream-cone 114\n",
      "116.iguana 115\n",
      "117.ipod 116\n",
      "118.iris 117\n",
      "119.jesus-christ 118\n",
      "120.joy-stick 119\n",
      "121.kangaroo-101 120\n",
      "122.kayak 121\n",
      "123.ketch-101 122\n",
      "124.killer-whale 123\n",
      "125.knife 124\n",
      "126.ladder 125\n",
      "127.laptop-101 126\n",
      "128.lathe 127\n",
      "129.leopards-101 128\n",
      "130.license-plate 129\n",
      "131.lightbulb 130\n",
      "132.light-house 131\n",
      "133.lightning 132\n",
      "134.llama-101 133\n",
      "135.mailbox 134\n",
      "136.mandolin 135\n",
      "137.mars 136\n",
      "138.mattress 137\n",
      "139.megaphone 138\n",
      "140.menorah-101 139\n",
      "141.microscope 140\n",
      "142.microwave 141\n",
      "143.minaret 142\n",
      "144.minotaur 143\n",
      "145.motorbikes-101 144\n",
      "146.mountain-bike 145\n",
      "147.mushroom 146\n",
      "148.mussels 147\n",
      "149.necktie 148\n",
      "150.octopus 149\n",
      "151.ostrich 150\n",
      "152.owl 151\n",
      "153.palm-pilot 152\n",
      "154.palm-tree 153\n",
      "155.paperclip 154\n",
      "156.paper-shredder 155\n",
      "157.pci-card 156\n",
      "158.penguin 157\n",
      "159.people 158\n",
      "160.pez-dispenser 159\n",
      "161.photocopier 160\n",
      "162.picnic-table 161\n",
      "163.playing-card 162\n",
      "164.porcupine 163\n",
      "165.pram 164\n",
      "166.praying-mantis 165\n",
      "167.pyramid 166\n",
      "168.raccoon 167\n",
      "169.radio-telescope 168\n",
      "170.rainbow 169\n",
      "171.refrigerator 170\n",
      "172.revolver-101 171\n",
      "173.rifle 172\n",
      "174.rotary-phone 173\n",
      "175.roulette-wheel 174\n",
      "176.saddle 175\n",
      "177.saturn 176\n",
      "178.school-bus 177\n",
      "179.scorpion-101 178\n",
      "180.screwdriver 179\n",
      "181.segway 180\n",
      "182.self-propelled-lawn-mower 181\n",
      "183.sextant 182\n",
      "184.sheet-music 183\n",
      "185.skateboard 184\n",
      "186.skunk 185\n",
      "187.skyscraper 186\n",
      "188.smokestack 187\n",
      "189.snail 188\n",
      "190.snake 189\n",
      "191.sneaker 190\n",
      "192.snowmobile 191\n",
      "193.soccer-ball 192\n",
      "194.socks 193\n",
      "195.soda-can 194\n",
      "196.spaghetti 195\n",
      "197.speed-boat 196\n",
      "198.spider 197\n",
      "199.spoon 198\n",
      "200.stained-glass 199\n",
      "201.starfish-101 200\n",
      "202.steering-wheel 201\n",
      "203.stirrups 202\n",
      "204.sunflower-101 203\n",
      "205.superman 204\n",
      "206.sushi 205\n",
      "207.swan 206\n",
      "208.swiss-army-knife 207\n",
      "209.sword 208\n",
      "210.syringe 209\n",
      "211.tambourine 210\n",
      "212.teapot 211\n",
      "213.teddy-bear 212\n",
      "214.teepee 213\n",
      "215.telephone-box 214\n",
      "216.tennis-ball 215\n",
      "217.tennis-court 216\n",
      "218.tennis-racket 217\n",
      "219.theodolite 218\n",
      "220.toaster 219\n",
      "221.tomato 220\n",
      "222.tombstone 221\n",
      "223.top-hat 222\n",
      "224.touring-bike 223\n",
      "225.tower-pisa 224\n",
      "226.traffic-light 225\n",
      "227.treadmill 226\n",
      "228.triceratops 227\n",
      "229.tricycle 228\n",
      "230.trilobite-101 229\n",
      "231.tripod 230\n",
      "232.t-shirt 231\n",
      "233.tuning-fork 232\n",
      "234.tweezer 233\n",
      "235.umbrella-101 234\n",
      "236.unicorn 235\n",
      "237.vcr 236\n",
      "238.video-projector 237\n",
      "239.washing-machine 238\n",
      "240.watch-101 239\n",
      "241.waterfall 240\n",
      "242.watermelon 241\n",
      "243.welding-mask 242\n",
      "244.wheelbarrow 243\n",
      "245.windmill 244\n",
      "246.wine-bottle 245\n",
      "247.xylophone 246\n",
      "248.yarmulke 247\n",
      "249.yo-yo 248\n",
      "250.zebra 249\n",
      "251.airplanes-101 250\n",
      "252.car-side-101 251\n",
      "253.faces-easy-101 252\n",
      "254.greyhound 253\n",
      "255.tennis-shoes 254\n",
      "256.toad 255\n",
      "257.clutter 256\n",
      "003.backpack 0\n",
      "004.baseball-bat 1\n",
      "005.baseball-glove 2\n",
      "008.bathtub 3\n",
      "011.billiards 4\n",
      "012.binoculars 5\n",
      "015.bonsai-101 6\n",
      "019.boxing-glove 7\n",
      "021.breadmaker 8\n",
      "040.cockroach 9\n",
      "043.coin 10\n",
      "044.comet 11\n",
      "046.computer-monitor 12\n",
      "063.electric-guitar-101 13\n",
      "064.elephant-101 14\n",
      "090.gorilla 15\n",
      "092.grapes 16\n",
      "096.hammock 17\n",
      "101.head-phones 18\n",
      "105.horse 19\n",
      "109.hot-tub 20\n",
      "117.ipod 21\n",
      "120.joy-stick 22\n",
      "126.ladder 23\n",
      "127.laptop-101 24\n",
      "129.leopards-101 25\n",
      "132.light-house 26\n",
      "133.lightning 27\n",
      "137.mars 28\n",
      "138.mattress 29\n",
      "143.minaret 30\n",
      "145.motorbikes-101 31\n",
      "147.mushroom 32\n",
      "148.mussels 33\n",
      "158.penguin 34\n",
      "159.people 35\n",
      "168.raccoon 36\n",
      "193.soccer-ball 37\n",
      "212.teapot 38\n",
      "214.teepee 39\n",
      "227.treadmill 40\n",
      "232.t-shirt 41\n",
      "234.tweezer 42\n",
      "240.watch-101 43\n",
      "251.airplanes-101 44\n",
      "253.faces-easy-101 45\n",
      "257.clutter 46\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p caltech_256_train_60\n",
    "for i in 256_ObjectCategories/*; do\n",
    "    c=`basename $i`\n",
    "    mkdir -p caltech_256_train_60/$c\n",
    "    for j in `ls $i/*.jpg | shuf | head -n 60`; do\n",
    "        mv $j caltech_256_train_60/$c/\n",
    "    done\n",
    "done\n",
    "\n",
    "python im2rec.py --list --recursive --resize 224 --num-thread 16 caltech-256-60-train caltech_256_train_60/\n",
    "python im2rec.py --list --recursive --resize 224 --num-thread 16 caltech-256-60-val 256_ObjectCategories/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A .lst file is a tab-separated file with three columns that contains a list of image files. The first column specifies the image index, the second column specifies the class label index for the image, and the third column specifies the relative path of the image file. The image index in the first column should be unique across all of the images. Here we make an image list file using the im2rec tool from MXNet. You can also create the .lst file in your own way. An example of .lst file is shown as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18052\t177.000000\t178.school-bus/178_0029.jpg\n",
      "8626\t84.000000\t085.goat/085_0097.jpg\n",
      "10023\t98.000000\t099.harpsichord/099_0055.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 ./caltech-256-60-train.lst > example.lst\n",
    "f = open('example.lst','r')\n",
    "lst_content = f.read()\n",
    "print(lst_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the data stored in S3 bucket. The image and lst files will be converted to RecordIO file internelly by the image classification algorithm. But if you wantrf to do the conversion, the following cell shows you how you would do it using the im2rec tool. Note that this is just an example of creating RecordIO files. We are not using them for training in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .rec file from /home/ec2-user/SageMaker/distributed-train-and-deploy/caltech-256-60-val.lst in /home/ec2-user/SageMaker/distributed-train-and-deploy\n",
      "time: 0.21176385879516602  count: 0\n",
      "time: 0.7262849807739258  count: 1000\n",
      "time: 0.9982683658599854  count: 2000\n",
      "time: 0.6415307521820068  count: 3000\n",
      "time: 0.5192985534667969  count: 4000\n",
      "Creating .rec file from /home/ec2-user/SageMaker/distributed-train-and-deploy/caltech-256-60-train.lst in /home/ec2-user/SageMaker/distributed-train-and-deploy\n",
      "time: 0.11991310119628906  count: 0\n",
      "time: 1.501725196838379  count: 1000\n",
      "time: 0.9733169078826904  count: 2000\n",
      "time: 1.4899826049804688  count: 3000\n",
      "time: 1.0999112129211426  count: 4000\n",
      "time: 1.2091081142425537  count: 5000\n",
      "time: 1.1355085372924805  count: 6000\n",
      "time: 0.9564504623413086  count: 7000\n",
      "time: 1.105994462966919  count: 8000\n",
      "time: 1.3307063579559326  count: 9000\n",
      "time: 1.0220484733581543  count: 10000\n",
      "time: 1.1079003810882568  count: 11000\n",
      "time: 1.0795586109161377  count: 12000\n",
      "time: 1.1630141735076904  count: 13000\n",
      "time: 1.1919212341308594  count: 14000\n",
      "time: 0.9474985599517822  count: 15000\n",
      "time: 4.5109124183654785  count: 16000\n",
      "time: 1.5754890441894531  count: 17000\n",
      "time: 0.7662351131439209  count: 18000\n",
      "time: 0.7332777976989746  count: 19000\n",
      "time: 0.6554925441741943  count: 20000\n",
      "time: 1.6377272605895996  count: 21000\n",
      "time: 0.9390401840209961  count: 22000\n",
      "time: 0.5264542102813721  count: 23000\n",
      "time: 0.31717681884765625  count: 24000\n",
      "time: 0.31996726989746094  count: 25000\n",
      "time: 0.2212047576904297  count: 26000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python im2rec.py --num-thread 16 caltech-256-60-val 256_ObjectCategories/\n",
    "python im2rec.py --num-thread 16 caltech-256-60-train caltech_256_train_60/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are bringing your own image files to train, please ensure that the .lst file follows the same format as described above. In order to train with the lst format interface, passing the lst file for both training and validation in the appropriate format is mandatory. Once we have the data available in the correct format for training, the next step is to upload the image and .lst file to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four channels: train, validation, train_lst, and validation_lst\n",
    "s3train = 's3://{}/image-classification/train/'.format(bucket)\n",
    "s3validation = 's3://{}/image-classification/validation/'.format(bucket)\n",
    "\n",
    "#Uplload Rec files\n",
    "!aws s3 cp caltech-256-60-train.rec $s3train --quiet\n",
    "!aws s3 cp caltech-256-60-val.rec $s3validation --quiet\n",
    "\n",
    "# upload the image files to train and validation channels\n",
    "#!aws s3 cp caltech_256_train_60 $s3train --recursive --quiet\n",
    "#!aws s3 cp 256_ObjectCategories $s3validation --recursive --quiet\n",
    "\n",
    "# upload the lst files to train_lst and validation_lst channels\n",
    "#!aws s3 cp caltech-256-60-train.lst $s3train_lst --quiet\n",
    "#!aws s3 cp caltech-256-60-val.lst $s3validation_lst --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get familiar with MXNet NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[14. 20.]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "# create a 1-dimensional array with a python list\n",
    "a = mx.nd.array([1,2,3])\n",
    "# create a 2-dimensional array with a nested python list\n",
    "b = mx.nd.array([[1,2,], [2,3], [3,4]])\n",
    "{'a.shape':a.shape, 'b.shape':b.shape}\n",
    "\n",
    "mx.nd.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 1x10 @cpu(0)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import gluon, autograd, ndarray\n",
    "\n",
    "def construct_net():\n",
    "    net = gluon.nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Conv2D(128, 3, activation='relu'))\n",
    "        net.add(gluon.nn.Flatten())\n",
    "        net.add(gluon.nn.Dense(128, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dense(64, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dense(10))\n",
    "    return net\n",
    "\n",
    "ctx = mx.cpu()\n",
    "net = construct_net()\n",
    "net.hybridize()\n",
    "net.initialize(mx.init.Xavier())\n",
    "\n",
    "arr = mx.nd.zeros(shape=(1,128,128,3))\n",
    "net(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! pip install gluoncv\n",
    "import gluoncv\n",
    "\n",
    "net = gluoncv.model_zoo.mobilenet_v2_1_0()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ResNet model\n",
    "\n",
    "\n",
    "\n",
    "## Training parameters\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include:\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html\n",
    "\n",
    "* **num_layers**: The number of layers (depth) for the network. Valid values: For data with large image size (224x224) positive integer in [18, 34, 50, 101, 152, 200] or for data with small image size (28x28) use [20, 32, 44, 56, 110]\n",
    "* **image_shape**: The input image dimensions,'num_channels, height, width', for the network. It should be no larger than the actual image size. The number of channels should be same as the actual image.\n",
    "* **num_classes**: This is the number of output classes for the new dataset. Imagenet was trained with 1000 output classes but the number of output classes can be changed for fine-tuning. For caltech, we use 257 because it has 256 object categories + 1 clutter class.\n",
    "* **num_training_samples**: This is the total number of training samples. It is set to 15240 for caltech dataset with the current split.\n",
    "* **mini_batch_size**: The number of training samples used for each mini batch. In distributed training, the number of training samples used per batch will be N * mini_batch_size where N is the number of hosts on which training is run.\n",
    "* **epochs**: Number of training epochs.\n",
    "* **learning_rate**: Learning rate for training.\n",
    "* **top_k**: Report the top-k accuracy during training.\n",
    "* **precision_dtype**: Training datatype precision (default: float32). If set to 'float16', the training will be done in mixed_precision mode and will be faster than float32 mode\n",
    "* **use_pretrained_model**:  Flag to use pre-trained model for training. If set to 1, then the pretrained model with the corresponding number of layers is loaded and used for training. Only the top FC layer are reinitialized with random weights. Otherwise, the network is trained from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job.\n",
    "### Training parameters\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include:\n",
    "\n",
    "* **Training instance count**: This is the number of instances on which to run the training. When the number of instances is greater than one, then the image classification algorithm will run in distributed settings. ml.p3.8xlarge has 4 x Nvidia V100.  https://aws.amazon.com/sagemaker/pricing/instance-types/\n",
    "* **Training instance type**: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training \n",
    "* **Output path**: This the s3 folder in which the training output is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "num_gpu = 4\n",
    "\n",
    "ic1 = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p3.8xlarge', \n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic1.set_hyperparameters(num_layers=101,\n",
    "                             image_shape = \"3,224,224\",\n",
    "                             num_classes=257,\n",
    "                             num_training_samples=15420,\n",
    "                             mini_batch_size=64 * num_gpu,\n",
    "                             epochs=2,\n",
    "                             optimizer='adam',\n",
    "                             momentum='.9',\n",
    "                             resize = 224,\n",
    "                             learning_rate=0.0002,\n",
    "                             top_k=5,\n",
    "                             kv_store='dist_sync',\n",
    "                             early_stopping=False,\n",
    "                             precision_dtype='float32',\n",
    "                             use_pretrained_model=1)\n",
    "\n",
    "\n",
    "# For Adam Optmizer specific hyperparameters\n",
    "# beta_1 Default value: 0.9\n",
    "# beta_2 Default value: 0.999\n",
    "# eps    Default value:  1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data specification\n",
    "Set the data type and channels used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# Since our data is in RecordIO format, we use these next 3 lines:\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated', \n",
    "                        content_type='application/x-recordio', s3_data_type='S3Prefix')\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated', \n",
    "                             content_type='application/x-recordio', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "\n",
    "\n",
    "## If we were using image files and lst format, we would use the following\n",
    "\n",
    "#train_lst = sagemaker.session.s3_input(s3train_lst, distribution='ShardedByS3Key',\n",
    "#                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "\n",
    "#validation_lst = sagemaker.session.s3_input(s3validation_lst, distribution='FullyReplicated',\n",
    "#                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "\n",
    "#data_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_lst, \n",
    "#                         'validation_lst': validation_lst}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Start training by calling the fit method of the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 04:20:34 Starting - Starting the training job...\n",
      "2020-08-13 04:20:36 Starting - Launching requested ML instances.........\n",
      "2020-08-13 04:22:20 Starting - Preparing the instances for training.........\n",
      "2020-08-13 04:23:52 Downloading - Downloading input data...\n",
      "2020-08-13 04:24:26 Training - Downloading the training image...\n",
      "2020-08-13 04:24:47 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:50 INFO 140123349768000] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:50 INFO 140123349768000] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.0002', u'top_k': u'5', u'optimizer': u'adam', u'image_shape': u'3,224,224', u'num_layers': u'101', u'kv_store': u'dist_sync', u'epochs': u'2', u'resize': u'224', u'early_stopping': u'False', u'precision_dtype': u'float32', u'num_classes': u'257', u'mini_batch_size': u'256', u'use_pretrained_model': u'1', u'momentum': u'.9', u'num_training_samples': u'15420'}\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:50 INFO 140123349768000] Final configuration: {u'top_k': u'5', u'optimizer': u'adam', u'learning_rate': u'0.0002', u'kv_store': u'dist_sync', u'epochs': u'2', u'lr_scheduler_factor': 0.1, u'num_layers': u'101', u'num_classes': u'257', u'precision_dtype': u'float32', u'mini_batch_size': u'256', u'early_stopping': u'False', u'resize': u'224', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': u'.9', u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'15420'}\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:50 INFO 140123349768000] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:50 INFO 140123349768000] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] multi_label: 0\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] num_layers: 101\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] epochs: 2\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] image resize size: 224\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] optimizer: adam\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] beta_1: 0.9\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] beta_2: 0.999\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] eps: 1e-08\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] learning_rate: 0.0002\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] num_training_samples: 15420\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] mini_batch_size: 256\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] num_classes: 257\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] kv_store: device\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] top_k: 5\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:51 INFO 140123349768000] --------------------\u001b[0m\n",
      "\u001b[34m[04:24:51] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.3437.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[04:24:51] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.3437.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:24:52 INFO 140123349768000] Setting number of threads: 31\u001b[0m\n",
      "\u001b[34m[04:25:11] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.3437.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:25:20 INFO 140123349768000] Epoch[0] Batch [20]#011Speed: 489.296 samples/sec#011accuracy=0.282924#011top_k_accuracy_5=0.453311\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:25:27 INFO 140123349768000] Epoch[0] Batch [40]#011Speed: 605.416 samples/sec#011accuracy=0.457031#011top_k_accuracy_5=0.643960\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:25:33 INFO 140123349768000] Epoch[0] Train-accuracy=0.549023\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:25:33 INFO 140123349768000] Epoch[0] Train-top_k_accuracy_5=0.728906\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:25:33 INFO 140123349768000] Epoch[0] Time cost=22.968\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:25:40 INFO 140123349768000] Epoch[0] Validation-accuracy=0.774479\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:25:40 INFO 140123349768000] Storing the best model with validation accuracy: 0.774479\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:25:41 INFO 140123349768000] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:25:47 INFO 140123349768000] Epoch[1] Batch [20]#011Speed: 759.962 samples/sec#011accuracy=0.857887#011top_k_accuracy_5=0.966518\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:25:54 INFO 140123349768000] Epoch[1] Batch [40]#011Speed: 776.077 samples/sec#011accuracy=0.880431#011top_k_accuracy_5=0.972466\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:26:00 INFO 140123349768000] Epoch[1] Train-accuracy=0.898372\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:26:00 INFO 140123349768000] Epoch[1] Train-top_k_accuracy_5=0.976888\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:26:00 INFO 140123349768000] Epoch[1] Time cost=19.254\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:26:07 INFO 140123349768000] Epoch[1] Validation-accuracy=0.825013\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:26:07 INFO 140123349768000] Storing the best model with validation accuracy: 0.825013\u001b[0m\n",
      "\u001b[34m[08/13/2020 04:26:07 INFO 140123349768000] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\n",
      "2020-08-13 04:26:13 Uploading - Uploading generated training model\n",
      "2020-08-13 04:26:40 Completed - Training job completed\n",
      "Training seconds: 168\n",
      "Billable seconds: 168\n",
      "CPU times: user 698 ms, sys: 37.5 ms, total: 735 ms\n",
      "Wall time: 6min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ic1.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managed Spot Training with MXNet\n",
    "\n",
    "For Managed Spot Training using MXNet we need to configure three things:\n",
    "1. Enable the `train_use_spot_instances` constructor arg - a simple self-explanatory boolean.\n",
    "2. Set the `train_max_wait` constructor arg - this is an int arg representing the amount of time you are willing to wait for Spot infrastructure to become available. Some instance types are harder to get at Spot prices and you may have to wait longer. You are not charged for time spent waiting for Spot infrastructure to become available, you're only charged for actual compute time spent once Spot instances have been successfully procured.\n",
    "3. Setup a `checkpoint_s3_uri` constructor arg. This arg will tell SageMaker an S3 location where to save checkpoints (assuming your algorithm has been modified to save checkpoints periodically). While not strictly necessary checkpointing is highly recommended for Manage Spot Training jobs due to the fact that Spot instances can be interrupted with short notice and using checkpoints to resume from the last interruption ensures you don't lose any progress made before the interruption.\n",
    "\n",
    "Feel free to toggle the `train_use_spot_instances` variable to see the effect of running the same job using regular (a.k.a. \"On Demand\") infrastructure.\n",
    "\n",
    "Note that `train_max_wait` can be set if and only if `train_use_spot_instances` is enabled and **must** be greater than or equal to `train_max_run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for running Managed Spot Training\n",
    "\n",
    "train_use_spot_instances = True\n",
    "train_max_wait = 3600\n",
    "import uuid\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_uri = 's3://{}/artifacts/mxnet-checkpoint-{}/'.format(bucket, checkpoint_suffix) if train_use_spot_instances else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "ic1spot = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p3.8xlarge', \n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 3600,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess,\n",
    "                                         train_use_spot_instances=train_use_spot_instances,\n",
    "                                         train_max_wait=train_max_wait,\n",
    "                                         checkpoint_s3_uri=checkpoint_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpu = 4\n",
    "\n",
    "ic1spot.set_hyperparameters(num_layers=101,\n",
    "                             image_shape = \"3,224,224\",\n",
    "                             num_classes=257,\n",
    "                             num_training_samples=15420,\n",
    "                             mini_batch_size=64 * num_gpu,\n",
    "                             epochs=20,\n",
    "                             optimizer='adam',\n",
    "                             momentum='.9',\n",
    "                             resize = 224,\n",
    "                             learning_rate=0.0002,\n",
    "                             top_k=5,\n",
    "                             kv_store='dist_sync',\n",
    "                             early_stopping=False,\n",
    "                             precision_dtype='float32',\n",
    "                             use_pretrained_model=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 12:06:39 Starting - Starting the training job...\n",
      "2020-08-13 12:06:41 Starting - Launching requested ML instances............\n",
      "2020-08-13 12:08:50 Starting - Preparing the instances for training...\n",
      "2020-08-13 12:09:30 Downloading - Downloading input data......\n",
      "2020-08-13 12:10:11 Training - Downloading the training image.....\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.0002', u'top_k': u'5', u'optimizer': u'adam', u'image_shape': u'3,224,224', u'num_layers': u'101', u'kv_store': u'dist_sync', u'epochs': u'20', u'resize': u'224', u'early_stopping': u'False', u'precision_dtype': u'float32', u'num_classes': u'257', u'mini_batch_size': u'256', u'use_pretrained_model': u'1', u'momentum': u'.9', u'num_training_samples': u'15420'}\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] Final configuration: {u'top_k': u'5', u'optimizer': u'adam', u'learning_rate': u'0.0002', u'kv_store': u'dist_sync', u'epochs': u'20', u'lr_scheduler_factor': 0.1, u'num_layers': u'101', u'num_classes': u'257', u'precision_dtype': u'float32', u'mini_batch_size': u'256', u'early_stopping': u'False', u'resize': u'224', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': u'.9', u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'15420'}\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] Checkpoint loading and saving are enabled.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] Checkpoint directories created successfully.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] No checkpoint hyperparameters found.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] Saving checkpoint hyperparameters\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] multi_label: 0\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] num_layers: 101\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] epochs: 20\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] image resize size: 224\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] optimizer: adam\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] beta_1: 0.9\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] beta_2: 0.999\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] eps: 1e-08\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] learning_rate: 0.0002\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] num_training_samples: 15420\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] mini_batch_size: 256\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] num_classes: 257\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] kv_store: device\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] top_k: 5\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:17 INFO 140537195575104] --------------------\u001b[0m\n",
      "\u001b[34m[12:11:17] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.3437.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[12:11:17] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.3437.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:18 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/hyperparameters.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:18 INFO 140537195575104] Setting number of threads: 31\u001b[0m\n",
      "\n",
      "2020-08-13 12:11:13 Training - Training image download completed. Training in progress.\u001b[34m[12:11:35] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.3437.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:45 INFO 140537195575104] Epoch[0] Batch [20]#011Speed: 488.178 samples/sec#011accuracy=0.281064#011top_k_accuracy_5=0.444754\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:52 INFO 140537195575104] Epoch[0] Batch [40]#011Speed: 603.230 samples/sec#011accuracy=0.452649#011top_k_accuracy_5=0.637005\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:58 INFO 140537195575104] Epoch[0] Train-accuracy=0.546159\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:58 INFO 140537195575104] Epoch[0] Train-top_k_accuracy_5=0.725065\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:11:58 INFO 140537195575104] Epoch[0] Time cost=23.136\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:00 INFO 140537195575104] Epoch[0] Validation-accuracy=0.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:00 INFO 140537195575104] checkpoint path /tmp/checkpoint_1\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:01 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_1/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:01 INFO 140537195575104] Storing the best model with validation accuracy: 0.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:01 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:01 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:01 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:01 INFO 140537195575104] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:01 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:08 INFO 140537195575104] Epoch[1] Batch [20]#011Speed: 751.395 samples/sec#011accuracy=0.853981#011top_k_accuracy_5=0.963170\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:14 INFO 140537195575104] Epoch[1] Batch [40]#011Speed: 769.011 samples/sec#011accuracy=0.879287#011top_k_accuracy_5=0.970084\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:20 INFO 140537195575104] Epoch[1] Train-accuracy=0.897786\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:20 INFO 140537195575104] Epoch[1] Train-top_k_accuracy_5=0.975716\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:20 INFO 140537195575104] Epoch[1] Time cost=19.482\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:23 INFO 140537195575104] Epoch[1] Validation-accuracy=0.000217\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:23 INFO 140537195575104] checkpoint path /tmp/checkpoint_2\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:23 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_2/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:23 INFO 140537195575104] Storing the best model with validation accuracy: 0.000217\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:23 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:23 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:23 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:23 INFO 140537195575104] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:23 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:30 INFO 140537195575104] Epoch[2] Batch [20]#011Speed: 752.013 samples/sec#011accuracy=0.973400#011top_k_accuracy_5=0.997210\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:37 INFO 140537195575104] Epoch[2] Batch [40]#011Speed: 770.051 samples/sec#011accuracy=0.979516#011top_k_accuracy_5=0.997809\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:43 INFO 140537195575104] Epoch[2] Train-accuracy=0.983268\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:43 INFO 140537195575104] Epoch[2] Train-top_k_accuracy_5=0.998307\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:43 INFO 140537195575104] Epoch[2] Time cost=19.476\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:45 INFO 140537195575104] Epoch[2] Validation-accuracy=0.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:46 INFO 140537195575104] checkpoint path /tmp/checkpoint_3\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:46 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_3/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:46 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:46 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:46 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:47 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:53 INFO 140537195575104] Epoch[3] Batch [20]#011Speed: 752.944 samples/sec#011accuracy=0.995350#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:12:59 INFO 140537195575104] Epoch[3] Batch [40]#011Speed: 768.998 samples/sec#011accuracy=0.996189#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:05 INFO 140537195575104] Epoch[3] Train-accuracy=0.996419\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:05 INFO 140537195575104] Epoch[3] Train-top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:05 INFO 140537195575104] Epoch[3] Time cost=19.487\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:07 INFO 140537195575104] Epoch[3] Validation-accuracy=0.000230\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:08 INFO 140537195575104] checkpoint path /tmp/checkpoint_1\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:08 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_1/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:08 INFO 140537195575104] Storing the best model with validation accuracy: 0.000230\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:08 INFO 140537195575104] Saved checkpoint to \"/opt/ml/model/image-classification-0004.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:09 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:09 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:09 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:09 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:15 INFO 140537195575104] Epoch[4] Batch [20]#011Speed: 757.201 samples/sec#011accuracy=0.997024#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:22 INFO 140537195575104] Epoch[4] Batch [40]#011Speed: 772.852 samples/sec#011accuracy=0.998190#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:28 INFO 140537195575104] Epoch[4] Train-accuracy=0.998437\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:28 INFO 140537195575104] Epoch[4] Train-top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:28 INFO 140537195575104] Epoch[4] Time cost=19.408\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:30 INFO 140537195575104] Epoch[4] Validation-accuracy=0.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:30 INFO 140537195575104] checkpoint path /tmp/checkpoint_2\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:30 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_2/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:31 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:31 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:31 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:32 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:37 INFO 140537195575104] Epoch[5] Batch [20]#011Speed: 752.670 samples/sec#011accuracy=0.999814#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:44 INFO 140537195575104] Epoch[5] Batch [40]#011Speed: 769.087 samples/sec#011accuracy=0.999714#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:50 INFO 140537195575104] Epoch[5] Train-accuracy=0.999609\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:50 INFO 140537195575104] Epoch[5] Train-top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:50 INFO 140537195575104] Epoch[5] Time cost=19.475\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:52 INFO 140537195575104] Epoch[5] Validation-accuracy=0.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:53 INFO 140537195575104] checkpoint path /tmp/checkpoint_3\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:53 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_3/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:54 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:54 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:54 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:13:54 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:00 INFO 140537195575104] Epoch[6] Batch [20]#011Speed: 752.854 samples/sec#011accuracy=0.999814#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:06 INFO 140537195575104] Epoch[6] Batch [40]#011Speed: 770.113 samples/sec#011accuracy=0.999809#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:12 INFO 140537195575104] Epoch[6] Train-accuracy=0.999674\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:12 INFO 140537195575104] Epoch[6] Train-top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:12 INFO 140537195575104] Epoch[6] Time cost=19.472\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:15 INFO 140537195575104] Epoch[6] Validation-accuracy=0.000217\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:15 INFO 140537195575104] checkpoint path /tmp/checkpoint_2\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:15 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_2/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:16 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:16 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:16 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:16 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:22 INFO 140537195575104] Epoch[7] Batch [20]#011Speed: 754.324 samples/sec#011accuracy=0.999814#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:29 INFO 140537195575104] Epoch[7] Batch [40]#011Speed: 769.424 samples/sec#011accuracy=0.999809#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:35 INFO 140537195575104] Epoch[7] Train-accuracy=0.999805\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:35 INFO 140537195575104] Epoch[7] Train-top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:35 INFO 140537195575104] Epoch[7] Time cost=19.462\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:37 INFO 140537195575104] Epoch[7] Validation-accuracy=0.000460\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:37 INFO 140537195575104] checkpoint path /tmp/checkpoint_3\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:37 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_3/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:37 INFO 140537195575104] Storing the best model with validation accuracy: 0.000460\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:38 INFO 140537195575104] Saved checkpoint to \"/opt/ml/model/image-classification-0008.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:38 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:38 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:38 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:39 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:45 INFO 140537195575104] Epoch[8] Batch [20]#011Speed: 753.266 samples/sec#011accuracy=1.000000#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:51 INFO 140537195575104] Epoch[8] Batch [40]#011Speed: 769.974 samples/sec#011accuracy=0.999905#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:57 INFO 140537195575104] Epoch[8] Train-accuracy=0.999870\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:57 INFO 140537195575104] Epoch[8] Train-top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:57 INFO 140537195575104] Epoch[8] Time cost=19.462\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:14:59 INFO 140537195575104] Epoch[8] Validation-accuracy=0.000434\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:00 INFO 140537195575104] checkpoint path /tmp/checkpoint_1\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:00 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_1/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:01 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:01 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:01 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:01 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:07 INFO 140537195575104] Epoch[9] Batch [20]#011Speed: 755.182 samples/sec#011accuracy=0.999442#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:13 INFO 140537195575104] Epoch[9] Batch [40]#011Speed: 771.888 samples/sec#011accuracy=0.999333#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:20 INFO 140537195575104] Epoch[9] Train-accuracy=0.999089\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:20 INFO 140537195575104] Epoch[9] Train-top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:20 INFO 140537195575104] Epoch[9] Time cost=19.427\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:22 INFO 140537195575104] Epoch[9] Validation-accuracy=0.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:22 INFO 140537195575104] checkpoint path /tmp/checkpoint_2\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:22 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_2/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:23 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:23 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:23 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:23 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:29 INFO 140537195575104] Epoch[10] Batch [20]#011Speed: 754.246 samples/sec#011accuracy=0.999628#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:36 INFO 140537195575104] Epoch[10] Batch [40]#011Speed: 770.703 samples/sec#011accuracy=0.999333#011top_k_accuracy_5=0.999905\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:42 INFO 140537195575104] Epoch[10] Train-accuracy=0.999284\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:42 INFO 140537195575104] Epoch[10] Train-top_k_accuracy_5=0.999935\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:42 INFO 140537195575104] Epoch[10] Time cost=19.467\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:44 INFO 140537195575104] Epoch[10] Validation-accuracy=0.000217\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:45 INFO 140537195575104] checkpoint path /tmp/checkpoint_1\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:45 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_1/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:45 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:45 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:45 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:46 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:52 INFO 140537195575104] Epoch[11] Batch [20]#011Speed: 751.135 samples/sec#011accuracy=0.997210#011top_k_accuracy_5=0.999628\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:15:58 INFO 140537195575104] Epoch[11] Batch [40]#011Speed: 770.678 samples/sec#011accuracy=0.993617#011top_k_accuracy_5=0.999619\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:04 INFO 140537195575104] Epoch[11] Train-accuracy=0.988411\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:04 INFO 140537195575104] Epoch[11] Train-top_k_accuracy_5=0.999349\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:04 INFO 140537195575104] Epoch[11] Time cost=19.438\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:06 INFO 140537195575104] Epoch[11] Validation-accuracy=0.000460\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:07 INFO 140537195575104] checkpoint path /tmp/checkpoint_2\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:07 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_2/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:08 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:08 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:08 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:08 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:14 INFO 140537195575104] Epoch[12] Batch [20]#011Speed: 752.943 samples/sec#011accuracy=0.954613#011top_k_accuracy_5=0.995350\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:20 INFO 140537195575104] Epoch[12] Batch [40]#011Speed: 769.807 samples/sec#011accuracy=0.938167#011top_k_accuracy_5=0.993236\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:27 INFO 140537195575104] Epoch[12] Train-accuracy=0.922331\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:27 INFO 140537195575104] Epoch[12] Train-top_k_accuracy_5=0.990560\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:27 INFO 140537195575104] Epoch[12] Time cost=19.476\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:29 INFO 140537195575104] Epoch[12] Validation-accuracy=0.000651\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:29 INFO 140537195575104] checkpoint path /tmp/checkpoint_3\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:29 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_3/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:29 INFO 140537195575104] Storing the best model with validation accuracy: 0.000651\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:30 INFO 140537195575104] Saved checkpoint to \"/opt/ml/model/image-classification-0013.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:30 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:30 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:30 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:30 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_3/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:36 INFO 140537195575104] Epoch[13] Batch [20]#011Speed: 753.844 samples/sec#011accuracy=0.896949#011top_k_accuracy_5=0.986235\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:43 INFO 140537195575104] Epoch[13] Batch [40]#011Speed: 772.365 samples/sec#011accuracy=0.911776#011top_k_accuracy_5=0.989329\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:49 INFO 140537195575104] Epoch[13] Train-accuracy=0.922070\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:49 INFO 140537195575104] Epoch[13] Train-top_k_accuracy_5=0.991406\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:49 INFO 140537195575104] Epoch[13] Time cost=19.402\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:51 INFO 140537195575104] Epoch[13] Validation-accuracy=0.000434\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:52 INFO 140537195575104] checkpoint path /tmp/checkpoint_1\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:52 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_1/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:52 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:52 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:52 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:53 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:16:59 INFO 140537195575104] Epoch[14] Batch [20]#011Speed: 752.235 samples/sec#011accuracy=0.965030#011top_k_accuracy_5=0.998512\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:05 INFO 140537195575104] Epoch[14] Batch [40]#011Speed: 768.247 samples/sec#011accuracy=0.971799#011top_k_accuracy_5=0.998857\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:11 INFO 140537195575104] Epoch[14] Train-accuracy=0.976823\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:11 INFO 140537195575104] Epoch[14] Train-top_k_accuracy_5=0.999219\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:11 INFO 140537195575104] Epoch[14] Time cost=19.481\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:14 INFO 140537195575104] Epoch[14] Validation-accuracy=0.000217\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:14 INFO 140537195575104] checkpoint path /tmp/checkpoint_2\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:14 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_2/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:15 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:15 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:15 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:15 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:21 INFO 140537195575104] Epoch[15] Batch [20]#011Speed: 751.615 samples/sec#011accuracy=0.990327#011top_k_accuracy_5=0.999442\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:28 INFO 140537195575104] Epoch[15] Batch [40]#011Speed: 769.456 samples/sec#011accuracy=0.990758#011top_k_accuracy_5=0.999428\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:34 INFO 140537195575104] Epoch[15] Train-accuracy=0.991927\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:34 INFO 140537195575104] Epoch[15] Train-top_k_accuracy_5=0.999544\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:34 INFO 140537195575104] Epoch[15] Time cost=19.467\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:36 INFO 140537195575104] Epoch[15] Validation-accuracy=0.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:36 INFO 140537195575104] checkpoint path /tmp/checkpoint_1\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:36 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_1/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:37 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:37 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:37 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:38 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:43 INFO 140537195575104] Epoch[16] Batch [20]#011Speed: 752.611 samples/sec#011accuracy=0.996094#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:50 INFO 140537195575104] Epoch[16] Batch [40]#011Speed: 770.146 samples/sec#011accuracy=0.996761#011top_k_accuracy_5=0.999905\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:56 INFO 140537195575104] Epoch[16] Train-accuracy=0.997266\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:56 INFO 140537195575104] Epoch[16] Train-top_k_accuracy_5=0.999935\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:56 INFO 140537195575104] Epoch[16] Time cost=19.459\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:58 INFO 140537195575104] Epoch[16] Validation-accuracy=0.000217\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:59 INFO 140537195575104] checkpoint path /tmp/checkpoint_2\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:17:59 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_2/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:00 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:00 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:00 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:00 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:06 INFO 140537195575104] Epoch[17] Batch [20]#011Speed: 752.841 samples/sec#011accuracy=0.999256#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:12 INFO 140537195575104] Epoch[17] Batch [40]#011Speed: 769.607 samples/sec#011accuracy=0.999333#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:18 INFO 140537195575104] Epoch[17] Train-accuracy=0.999349\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:18 INFO 140537195575104] Epoch[17] Train-top_k_accuracy_5=0.999935\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:18 INFO 140537195575104] Epoch[17] Time cost=19.471\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:21 INFO 140537195575104] Epoch[17] Validation-accuracy=0.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:21 INFO 140537195575104] checkpoint path /tmp/checkpoint_1\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:21 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_1/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:22 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:22 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:22 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:22 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:28 INFO 140537195575104] Epoch[18] Batch [20]#011Speed: 755.086 samples/sec#011accuracy=0.999814#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:35 INFO 140537195575104] Epoch[18] Batch [40]#011Speed: 772.162 samples/sec#011accuracy=0.999714#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:41 INFO 140537195575104] Epoch[18] Train-accuracy=0.999674\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:41 INFO 140537195575104] Epoch[18] Train-top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:41 INFO 140537195575104] Epoch[18] Time cost=19.419\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:43 INFO 140537195575104] Epoch[18] Validation-accuracy=0.000217\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:43 INFO 140537195575104] checkpoint path /tmp/checkpoint_2\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:44 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_2/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:44 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:44 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:44 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:45 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_2/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:51 INFO 140537195575104] Epoch[19] Batch [20]#011Speed: 751.952 samples/sec#011accuracy=1.000000#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:18:57 INFO 140537195575104] Epoch[19] Batch [40]#011Speed: 771.158 samples/sec#011accuracy=0.999905#011top_k_accuracy_5=1.000000\u001b[0m\n",
      "\n",
      "2020-08-13 12:19:12 Uploading - Uploading generated training model\u001b[34m[08/13/2020 12:19:03 INFO 140537195575104] Epoch[19] Train-accuracy=0.999805\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:19:03 INFO 140537195575104] Epoch[19] Train-top_k_accuracy_5=1.000000\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:19:03 INFO 140537195575104] Epoch[19] Time cost=19.443\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:19:05 INFO 140537195575104] Epoch[19] Validation-accuracy=0.000460\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:19:06 INFO 140537195575104] checkpoint path /tmp/checkpoint_1\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:19:06 INFO 140537195575104] Saved checkpoint to \"/tmp/checkpoint_1/checkpoint-0000.params\"\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:19:07 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/model-shapes.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:19:07 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-symbol.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:19:07 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/metadata.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:19:07 INFO 140537195575104] Terminating checkpoint state checker.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:19:07 INFO 140535404672768] Checkpoint: file '/opt/ml/checkpoints/checkpoint_1/checkpoint-0000.params' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[08/13/2020 12:19:07 INFO 140537195575104] Checkpoint state checker terminated.\u001b[0m\n",
      "\n",
      "2020-08-13 12:19:40 Completed - Training job completed\n",
      "Training seconds: 610\n",
      "Billable seconds: 183\n",
      "Managed Spot Training savings: 70.0%\n",
      "CPU times: user 1.46 s, sys: 50.1 ms, total: 1.51 s\n",
      "Wall time: 13min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ic1spot.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Savings\n",
    "Towards the end of the job you should see two lines of output printed:\n",
    "\n",
    "- `Training seconds: X` : This is the actual compute-time your training job spent\n",
    "- `Billable seconds: Y` : This is the time you will be billed for after Spot discounting is applied.\n",
    "\n",
    "If you enabled the `train_use_spot_instances` var then you should see a notable difference between `X` and `Y` signifying the cost savings you will get for having chosen Managed Spot Training. This should be reflected in an additional line:\n",
    "- `Managed Spot Training savings: (1-Y/X)*100 %`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification Hyperameters Tuning\n",
    "\n",
    "Once we've defined our estimator we can specify the hyperparameters we'd like to tune and their possible values. We have three different types of hyperparameters.\n",
    "\n",
    "Categorical parameters need to take one value from a discrete set. We define this by passing the list of possible values to CategoricalParameter(list)\n",
    "Continuous parameters can take any real number value between the minimum and maximum value, defined by ContinuousParameter(min, max)\n",
    "Integer parameters can take any integer value between the minimum and maximum value, defined by IntegerParameter(min, max)\n",
    "Note, if possible, it's almost always best to specify a value as the least restrictive type. For example, tuning thresh as a continuous value between 0.01 and 0.2 is likely to yield a better result than tuning as a categorical parameter with possible values of 0.01, 0.1, 0.15, or 0.2.\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.debugger import Rule, CollectionConfig, rule_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugger Rules for Deep Learning\n",
    "\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanishing_gradient_rule = Rule.sagemaker(rule_configs.vanishing_gradient())\n",
    "\n",
    "ichpo = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p3.2xlarge', \n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 3600,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess,                                    \n",
    "                                      \n",
    "                                         ## Rule Parameter\n",
    "                                         rules = [vanishing_gradient_rule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpu = 4\n",
    "\n",
    "ichpo.set_hyperparameters(num_layers=101,\n",
    "                             image_shape = \"3,224,224\",\n",
    "                             num_classes=257,\n",
    "                             num_training_samples=15420,\n",
    "                             mini_batch_size=64 * num_gpu,\n",
    "                             optimizer='adam',\n",
    "                             momentum='.9',\n",
    "                             resize = 224,\n",
    "                             learning_rate=0.0002,\n",
    "                             epochs=200,\n",
    "                             top_k=5,\n",
    "                             early_stopping=True,\n",
    "                             use_pretrained_model=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'Adam']),\n",
    "                         'learning_rate': ContinuousParameter(0.001, 0.002),\n",
    "                         'momentum': ContinuousParameter(.6, .99)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll specify the objective metric that we'd like to tune and its definition. This includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'Validation-accuracy'\n",
    "metric_definitions = [{'Name': 'Validation-accuracy',\n",
    "                       'Regex': 'Validation-accuracy=([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(ichpo,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ichpo.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy The Model\n",
    "\n",
    "***\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the topic mixture representing a given document. \n",
    "\n",
    "This section involves several steps,\n",
    "\n",
    "1. [Create Model](#CreateModel) - Create model for the training output\n",
    "1. [Host the model for real-time inference with EI](#HostTheModel) - Create an inference with EI and perform real-time inference using EI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "We now create a SageMaker Model from the training output. Using the model we will create an Endpoint Configuration to start an endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic1_classifier = ic1.deploy(initial_instance_count = 2,\n",
    "                          instance_type = 'ml.p3.2xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iceia_classifier = ic1spot.deploy(initial_instance_count = 2,\n",
    "                             instance_type = 'ml.m5.large',\n",
    "                             accelerator_type = 'ml.eia2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O /tmp/test-1.jpg http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/080.frog/080_0001.jpg\n",
    "!wget -O /tmp/test-2.jpg http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/002.american-flag/002_0004.jpg\n",
    "!wget -O /tmp/test-3.jpg http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/037.chess-board/037_0006.jpg\n",
    "!wget -O /tmp/test-4.jpg http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/086.golden-gate-bridge/086_0010.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name1 = '/tmp/test-1.jpg'\n",
    "file_name2 = '/tmp/test-2.jpg'\n",
    "file_name3 = '/tmp/test-3.jpg'\n",
    "file_name4 = '/tmp/test-4.jpg'\n",
    "# test image\n",
    "from IPython.display import Image\n",
    "Image(file_name1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using our Training Model\n",
    "\n",
    "Evaluate the image through the network for inteference. The network outputs class probabilities and typically, one selects the class with the maximum probability as the final class output.\n",
    "\n",
    "**Note:** The output class detected by the network may not be accurate in this example. To limit the time taken and cost of training, we have trained the model only for 5 epochs. If the network is trained for more epochs (say 20), then the output class will be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(file_name1, 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "    \n",
    "ic1_classifier.content_type = 'application/x-image'\n",
    "iceia_classifier.content_type = 'application/x-image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_ic1 = json.loads(ic1_classifier.predict(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_iceia = json.loads(iceia_classifier.predict(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result will output the probabilities for all classes\n",
    "# find the class with maximum probability and print the class index\n",
    "index1 = np.argmax(result_ic1)\n",
    "indexeia = np.argmax(result_iceia)\n",
    "\n",
    "object_categories = ['ak47', 'american-flag', 'backpack', 'baseball-bat', 'baseball-glove', 'basketball-hoop', 'bat', 'bathtub', 'bear', 'beer-mug', 'billiards', 'binoculars', 'birdbath', 'blimp', 'bonsai-101', 'boom-box', 'bowling-ball', 'bowling-pin', 'boxing-glove', 'brain-101', 'breadmaker', 'buddha-101', 'bulldozer', 'butterfly', 'cactus', 'cake', 'calculator', 'camel', 'cannon', 'canoe', 'car-tire', 'cartman', 'cd', 'centipede', 'cereal-box', 'chandelier-101', 'chess-board', 'chimp', 'chopsticks', 'cockroach', 'coffee-mug', 'coffin', 'coin', 'comet', 'computer-keyboard', 'computer-monitor', 'computer-mouse', 'conch', 'cormorant', 'covered-wagon', 'cowboy-hat', 'crab-101', 'desk-globe', 'diamond-ring', 'dice', 'dog', 'dolphin-101', 'doorknob', 'drinking-straw', 'duck', 'dumb-bell', 'eiffel-tower', 'electric-guitar-101', 'elephant-101', 'elk', 'ewer-101', 'eyeglasses', 'fern', 'fighter-jet', 'fire-extinguisher', 'fire-hydrant', 'fire-truck', 'fireworks', 'flashlight', 'floppy-disk', 'football-helmet', 'french-horn', 'fried-egg', 'frisbee', 'frog', 'frying-pan', 'galaxy', 'gas-pump', 'giraffe', 'goat', 'golden-gate-bridge', 'goldfish', 'golf-ball', 'goose', 'gorilla', 'grand-piano-101', 'grapes', 'grasshopper', 'guitar-pick', 'hamburger', 'hammock', 'harmonica', 'harp', 'harpsichord', 'hawksbill-101', 'head-phones', 'helicopter-101', 'hibiscus', 'homer-simpson', 'horse', 'horseshoe-crab', 'hot-air-balloon', 'hot-dog', 'hot-tub', 'hourglass', 'house-fly', 'human-skeleton', 'hummingbird', 'ibis-101', 'ice-cream-cone', 'iguana', 'ipod', 'iris', 'jesus-christ', 'joy-stick', 'kangaroo-101', 'kayak', 'ketch-101', 'killer-whale', 'knife', 'ladder', 'laptop-101', 'lathe', 'leopards-101', 'license-plate', 'lightbulb', 'light-house', 'lightning', 'llama-101', 'mailbox', 'mandolin', 'mars', 'mattress', 'megaphone', 'menorah-101', 'microscope', 'microwave', 'minaret', 'minotaur', 'motorbikes-101', 'mountain-bike', 'mushroom', 'mussels', 'necktie', 'octopus', 'ostrich', 'owl', 'palm-pilot', 'palm-tree', 'paperclip', 'paper-shredder', 'pci-card', 'penguin', 'people', 'pez-dispenser', 'photocopier', 'picnic-table', 'playing-card', 'porcupine', 'pram', 'praying-mantis', 'pyramid', 'raccoon', 'radio-telescope', 'rainbow', 'refrigerator', 'revolver-101', 'rifle', 'rotary-phone', 'roulette-wheel', 'saddle', 'saturn', 'school-bus', 'scorpion-101', 'screwdriver', 'segway', 'self-propelled-lawn-mower', 'sextant', 'sheet-music', 'skateboard', 'skunk', 'skyscraper', 'smokestack', 'snail', 'snake', 'sneaker', 'snowmobile', 'soccer-ball', 'socks', 'soda-can', 'spaghetti', 'speed-boat', 'spider', 'spoon', 'stained-glass', 'starfish-101', 'steering-wheel', 'stirrups', 'sunflower-101', 'superman', 'sushi', 'swan', 'swiss-army-knife', 'sword', 'syringe', 'tambourine', 'teapot', 'teddy-bear', 'teepee', 'telephone-box', 'tennis-ball', 'tennis-court', 'tennis-racket', 'theodolite', 'toaster', 'tomato', 'tombstone', 'top-hat', 'touring-bike', 'tower-pisa', 'traffic-light', 'treadmill', 'triceratops', 'tricycle', 'trilobite-101', 'tripod', 't-shirt', 'tuning-fork', 'tweezer', 'umbrella-101', 'unicorn', 'vcr', 'video-projector', 'washing-machine', 'watch-101', 'waterfall', 'watermelon', 'welding-mask', 'wheelbarrow', 'windmill', 'wine-bottle', 'xylophone', 'yarmulke', 'yo-yo', 'zebra', 'airplanes-101', 'car-side-101', 'faces-easy-101', 'greyhound', 'tennis-shoes', 'toad', 'clutter']\n",
    "\n",
    "print(\"Result: label - \" + object_categories[index1] + \", probability - \" + str(result_ic1[index1]))\n",
    "print(\"Result: label - \" + object_categories[indexeia] + \", probability - \" + str(result_iceia[indexeia]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
