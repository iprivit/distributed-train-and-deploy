{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Using SageMaker Image Classification with Amazon Elastic Inference\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "  1. [Permissions and environment variables](#Permissions-and-environment-variables)\n",
    "3. [Training the ResNet model](#Training-the-ResNet-model)\n",
    "4. [Deploy The Model](#Deploy-the-model)\n",
    "  1. [Create model](#Create-model)\n",
    "  3. [Real-time inference](#Real-time-inference)\n",
    "    1. [Create endpoint configuration](#Create-endpoint-configuration) \n",
    "    2. [Create endpoint](#Create-endpoint) \n",
    "    3. [Perform inference](#Perform-inference) \n",
    "    4. [Clean up](#Clean-up)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to enable and use Amazon Elastic Inference (EI) for real-time inference with SageMaker Image Classification algorithm.\n",
    "\n",
    "Amazon Elastic Inference (EI) is a service that provides cost-efficient hardware acceleration meant for inferences in AWS. For more information please visit: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n",
    "\n",
    "This notebook is an adaption of the SageMaker Image Classification's [end-to-end notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-fulltraining-highlevel.ipynb), with modifications showing the changes needed to use EI for real-time inference with SageMaker Image Classification algorithm.\n",
    "\n",
    "In this demo, we will use the Amazon SageMaker image classification algorithm to train on the [caltech-256 dataset](http://www.vision.caltech.edu/Image_Datasets/Caltech256/). \n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication to AWS services. There are three parts to this:\n",
    "\n",
    "* The roles used to give learning and hosting access to your data. This will automatically be obtained from the role used to start the notebook\n",
    "* The S3 bucket that you want to use for training and model data\n",
    "* The Amazon SageMaker Image Classification docker image which need not be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket=sess.default_bucket()\n",
    "prefix = 'ic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Download the data and transfer to S3 for use in training. In this demo, we are using [Caltech-256](http://www.vision.caltech.edu/Image_Datasets/Caltech256/) dataset, which contains 30608 images of 256 objects. For the training and validation data, we follow the splitting scheme in this MXNet [example](https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/data/caltech256.sh). In particular, it randomly selects 60 images per class for training, and uses the remaining data for validation. The algorithm takes `RecordIO` file as input. The user can also provide the image files as input, which will be converted into `RecordIO` format using MXNet's [im2rec](https://mxnet.incubator.apache.org/how_to/recordio.html?highlight=im2rec) tool. It takes around 50 seconds to convert the entire Caltech-256 dataset (~1.2GB) on a p2.xlarge instance. However, for this demo, we will use record io format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import urllib.request\n",
    "import boto3\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "        \n",
    "def upload_to_s3(channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "\n",
    "# caltech-256 (RecordIO format)\n",
    "#download('http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec')\n",
    "#download('http://data.mxnet.io/data/caltech-256/caltech-256-60-val.rec')\n",
    "\n",
    "# Caltech-256 image files\n",
    "download('http://www.vision.caltech.edu/Image_Datasets/Caltech256/256_ObjectCategories.tar')\n",
    "!tar -xf 256_ObjectCategories.tar\n",
    "\n",
    "# Tool for creating lst file\n",
    "download('https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001.ak47 0\n",
      "002.american-flag 1\n",
      "003.backpack 2\n",
      "004.baseball-bat 3\n",
      "005.baseball-glove 4\n",
      "006.basketball-hoop 5\n",
      "007.bat 6\n",
      "008.bathtub 7\n",
      "009.bear 8\n",
      "010.beer-mug 9\n",
      "011.billiards 10\n",
      "012.binoculars 11\n",
      "013.birdbath 12\n",
      "014.blimp 13\n",
      "015.bonsai-101 14\n",
      "016.boom-box 15\n",
      "017.bowling-ball 16\n",
      "018.bowling-pin 17\n",
      "019.boxing-glove 18\n",
      "020.brain-101 19\n",
      "021.breadmaker 20\n",
      "022.buddha-101 21\n",
      "023.bulldozer 22\n",
      "024.butterfly 23\n",
      "025.cactus 24\n",
      "026.cake 25\n",
      "027.calculator 26\n",
      "028.camel 27\n",
      "029.cannon 28\n",
      "030.canoe 29\n",
      "031.car-tire 30\n",
      "032.cartman 31\n",
      "033.cd 32\n",
      "034.centipede 33\n",
      "035.cereal-box 34\n",
      "036.chandelier-101 35\n",
      "037.chess-board 36\n",
      "038.chimp 37\n",
      "039.chopsticks 38\n",
      "040.cockroach 39\n",
      "041.coffee-mug 40\n",
      "042.coffin 41\n",
      "043.coin 42\n",
      "044.comet 43\n",
      "045.computer-keyboard 44\n",
      "046.computer-monitor 45\n",
      "047.computer-mouse 46\n",
      "048.conch 47\n",
      "049.cormorant 48\n",
      "050.covered-wagon 49\n",
      "051.cowboy-hat 50\n",
      "052.crab-101 51\n",
      "053.desk-globe 52\n",
      "054.diamond-ring 53\n",
      "055.dice 54\n",
      "056.dog 55\n",
      "057.dolphin-101 56\n",
      "058.doorknob 57\n",
      "059.drinking-straw 58\n",
      "060.duck 59\n",
      "061.dumb-bell 60\n",
      "062.eiffel-tower 61\n",
      "063.electric-guitar-101 62\n",
      "064.elephant-101 63\n",
      "065.elk 64\n",
      "066.ewer-101 65\n",
      "067.eyeglasses 66\n",
      "068.fern 67\n",
      "069.fighter-jet 68\n",
      "070.fire-extinguisher 69\n",
      "071.fire-hydrant 70\n",
      "072.fire-truck 71\n",
      "073.fireworks 72\n",
      "074.flashlight 73\n",
      "075.floppy-disk 74\n",
      "076.football-helmet 75\n",
      "077.french-horn 76\n",
      "078.fried-egg 77\n",
      "079.frisbee 78\n",
      "080.frog 79\n",
      "081.frying-pan 80\n",
      "082.galaxy 81\n",
      "083.gas-pump 82\n",
      "084.giraffe 83\n",
      "085.goat 84\n",
      "086.golden-gate-bridge 85\n",
      "087.goldfish 86\n",
      "088.golf-ball 87\n",
      "089.goose 88\n",
      "090.gorilla 89\n",
      "091.grand-piano-101 90\n",
      "092.grapes 91\n",
      "093.grasshopper 92\n",
      "094.guitar-pick 93\n",
      "095.hamburger 94\n",
      "096.hammock 95\n",
      "097.harmonica 96\n",
      "098.harp 97\n",
      "099.harpsichord 98\n",
      "100.hawksbill-101 99\n",
      "101.head-phones 100\n",
      "102.helicopter-101 101\n",
      "103.hibiscus 102\n",
      "104.homer-simpson 103\n",
      "105.horse 104\n",
      "106.horseshoe-crab 105\n",
      "107.hot-air-balloon 106\n",
      "108.hot-dog 107\n",
      "109.hot-tub 108\n",
      "110.hourglass 109\n",
      "111.house-fly 110\n",
      "112.human-skeleton 111\n",
      "113.hummingbird 112\n",
      "114.ibis-101 113\n",
      "115.ice-cream-cone 114\n",
      "116.iguana 115\n",
      "117.ipod 116\n",
      "118.iris 117\n",
      "119.jesus-christ 118\n",
      "120.joy-stick 119\n",
      "121.kangaroo-101 120\n",
      "122.kayak 121\n",
      "123.ketch-101 122\n",
      "124.killer-whale 123\n",
      "125.knife 124\n",
      "126.ladder 125\n",
      "127.laptop-101 126\n",
      "128.lathe 127\n",
      "129.leopards-101 128\n",
      "130.license-plate 129\n",
      "131.lightbulb 130\n",
      "132.light-house 131\n",
      "133.lightning 132\n",
      "134.llama-101 133\n",
      "135.mailbox 134\n",
      "136.mandolin 135\n",
      "137.mars 136\n",
      "138.mattress 137\n",
      "139.megaphone 138\n",
      "140.menorah-101 139\n",
      "141.microscope 140\n",
      "142.microwave 141\n",
      "143.minaret 142\n",
      "144.minotaur 143\n",
      "145.motorbikes-101 144\n",
      "146.mountain-bike 145\n",
      "147.mushroom 146\n",
      "148.mussels 147\n",
      "149.necktie 148\n",
      "150.octopus 149\n",
      "151.ostrich 150\n",
      "152.owl 151\n",
      "153.palm-pilot 152\n",
      "154.palm-tree 153\n",
      "155.paperclip 154\n",
      "156.paper-shredder 155\n",
      "157.pci-card 156\n",
      "158.penguin 157\n",
      "159.people 158\n",
      "160.pez-dispenser 159\n",
      "161.photocopier 160\n",
      "162.picnic-table 161\n",
      "163.playing-card 162\n",
      "164.porcupine 163\n",
      "165.pram 164\n",
      "166.praying-mantis 165\n",
      "167.pyramid 166\n",
      "168.raccoon 167\n",
      "169.radio-telescope 168\n",
      "170.rainbow 169\n",
      "171.refrigerator 170\n",
      "172.revolver-101 171\n",
      "173.rifle 172\n",
      "174.rotary-phone 173\n",
      "175.roulette-wheel 174\n",
      "176.saddle 175\n",
      "177.saturn 176\n",
      "178.school-bus 177\n",
      "179.scorpion-101 178\n",
      "180.screwdriver 179\n",
      "181.segway 180\n",
      "182.self-propelled-lawn-mower 181\n",
      "183.sextant 182\n",
      "184.sheet-music 183\n",
      "185.skateboard 184\n",
      "186.skunk 185\n",
      "187.skyscraper 186\n",
      "188.smokestack 187\n",
      "189.snail 188\n",
      "190.snake 189\n",
      "191.sneaker 190\n",
      "192.snowmobile 191\n",
      "193.soccer-ball 192\n",
      "194.socks 193\n",
      "195.soda-can 194\n",
      "196.spaghetti 195\n",
      "197.speed-boat 196\n",
      "198.spider 197\n",
      "199.spoon 198\n",
      "200.stained-glass 199\n",
      "201.starfish-101 200\n",
      "202.steering-wheel 201\n",
      "203.stirrups 202\n",
      "204.sunflower-101 203\n",
      "205.superman 204\n",
      "206.sushi 205\n",
      "207.swan 206\n",
      "208.swiss-army-knife 207\n",
      "209.sword 208\n",
      "210.syringe 209\n",
      "211.tambourine 210\n",
      "212.teapot 211\n",
      "213.teddy-bear 212\n",
      "214.teepee 213\n",
      "215.telephone-box 214\n",
      "216.tennis-ball 215\n",
      "217.tennis-court 216\n",
      "218.tennis-racket 217\n",
      "219.theodolite 218\n",
      "220.toaster 219\n",
      "221.tomato 220\n",
      "222.tombstone 221\n",
      "223.top-hat 222\n",
      "224.touring-bike 223\n",
      "225.tower-pisa 224\n",
      "226.traffic-light 225\n",
      "227.treadmill 226\n",
      "228.triceratops 227\n",
      "229.tricycle 228\n",
      "230.trilobite-101 229\n",
      "231.tripod 230\n",
      "232.t-shirt 231\n",
      "233.tuning-fork 232\n",
      "234.tweezer 233\n",
      "235.umbrella-101 234\n",
      "236.unicorn 235\n",
      "237.vcr 236\n",
      "238.video-projector 237\n",
      "239.washing-machine 238\n",
      "240.watch-101 239\n",
      "241.waterfall 240\n",
      "242.watermelon 241\n",
      "243.welding-mask 242\n",
      "244.wheelbarrow 243\n",
      "245.windmill 244\n",
      "246.wine-bottle 245\n",
      "247.xylophone 246\n",
      "248.yarmulke 247\n",
      "249.yo-yo 248\n",
      "250.zebra 249\n",
      "251.airplanes-101 250\n",
      "252.car-side-101 251\n",
      "253.faces-easy-101 252\n",
      "254.greyhound 253\n",
      "255.tennis-shoes 254\n",
      "256.toad 255\n",
      "257.clutter 256\n",
      "001.ak47 0\n",
      "002.american-flag 1\n",
      "003.backpack 2\n",
      "004.baseball-bat 3\n",
      "005.baseball-glove 4\n",
      "006.basketball-hoop 5\n",
      "007.bat 6\n",
      "008.bathtub 7\n",
      "009.bear 8\n",
      "010.beer-mug 9\n",
      "011.billiards 10\n",
      "012.binoculars 11\n",
      "013.birdbath 12\n",
      "014.blimp 13\n",
      "015.bonsai-101 14\n",
      "016.boom-box 15\n",
      "017.bowling-ball 16\n",
      "018.bowling-pin 17\n",
      "019.boxing-glove 18\n",
      "020.brain-101 19\n",
      "021.breadmaker 20\n",
      "022.buddha-101 21\n",
      "023.bulldozer 22\n",
      "024.butterfly 23\n",
      "025.cactus 24\n",
      "026.cake 25\n",
      "027.calculator 26\n",
      "028.camel 27\n",
      "029.cannon 28\n",
      "030.canoe 29\n",
      "031.car-tire 30\n",
      "032.cartman 31\n",
      "033.cd 32\n",
      "034.centipede 33\n",
      "035.cereal-box 34\n",
      "036.chandelier-101 35\n",
      "037.chess-board 36\n",
      "038.chimp 37\n",
      "039.chopsticks 38\n",
      "040.cockroach 39\n",
      "041.coffee-mug 40\n",
      "042.coffin 41\n",
      "043.coin 42\n",
      "044.comet 43\n",
      "045.computer-keyboard 44\n",
      "046.computer-monitor 45\n",
      "047.computer-mouse 46\n",
      "048.conch 47\n",
      "049.cormorant 48\n",
      "050.covered-wagon 49\n",
      "051.cowboy-hat 50\n",
      "052.crab-101 51\n",
      "053.desk-globe 52\n",
      "054.diamond-ring 53\n",
      "055.dice 54\n",
      "056.dog 55\n",
      "057.dolphin-101 56\n",
      "058.doorknob 57\n",
      "059.drinking-straw 58\n",
      "060.duck 59\n",
      "061.dumb-bell 60\n",
      "062.eiffel-tower 61\n",
      "063.electric-guitar-101 62\n",
      "064.elephant-101 63\n",
      "065.elk 64\n",
      "066.ewer-101 65\n",
      "067.eyeglasses 66\n",
      "068.fern 67\n",
      "069.fighter-jet 68\n",
      "070.fire-extinguisher 69\n",
      "071.fire-hydrant 70\n",
      "072.fire-truck 71\n",
      "073.fireworks 72\n",
      "074.flashlight 73\n",
      "075.floppy-disk 74\n",
      "076.football-helmet 75\n",
      "077.french-horn 76\n",
      "078.fried-egg 77\n",
      "079.frisbee 78\n",
      "080.frog 79\n",
      "081.frying-pan 80\n",
      "082.galaxy 81\n",
      "083.gas-pump 82\n",
      "084.giraffe 83\n",
      "085.goat 84\n",
      "086.golden-gate-bridge 85\n",
      "087.goldfish 86\n",
      "088.golf-ball 87\n",
      "089.goose 88\n",
      "090.gorilla 89\n",
      "091.grand-piano-101 90\n",
      "092.grapes 91\n",
      "093.grasshopper 92\n",
      "094.guitar-pick 93\n",
      "095.hamburger 94\n",
      "096.hammock 95\n",
      "097.harmonica 96\n",
      "098.harp 97\n",
      "099.harpsichord 98\n",
      "100.hawksbill-101 99\n",
      "101.head-phones 100\n",
      "102.helicopter-101 101\n",
      "103.hibiscus 102\n",
      "104.homer-simpson 103\n",
      "105.horse 104\n",
      "106.horseshoe-crab 105\n",
      "107.hot-air-balloon 106\n",
      "108.hot-dog 107\n",
      "109.hot-tub 108\n",
      "110.hourglass 109\n",
      "111.house-fly 110\n",
      "112.human-skeleton 111\n",
      "113.hummingbird 112\n",
      "114.ibis-101 113\n",
      "115.ice-cream-cone 114\n",
      "116.iguana 115\n",
      "117.ipod 116\n",
      "118.iris 117\n",
      "119.jesus-christ 118\n",
      "120.joy-stick 119\n",
      "121.kangaroo-101 120\n",
      "122.kayak 121\n",
      "123.ketch-101 122\n",
      "124.killer-whale 123\n",
      "125.knife 124\n",
      "126.ladder 125\n",
      "127.laptop-101 126\n",
      "128.lathe 127\n",
      "129.leopards-101 128\n",
      "130.license-plate 129\n",
      "131.lightbulb 130\n",
      "132.light-house 131\n",
      "133.lightning 132\n",
      "134.llama-101 133\n",
      "135.mailbox 134\n",
      "136.mandolin 135\n",
      "137.mars 136\n",
      "138.mattress 137\n",
      "139.megaphone 138\n",
      "140.menorah-101 139\n",
      "141.microscope 140\n",
      "142.microwave 141\n",
      "143.minaret 142\n",
      "144.minotaur 143\n",
      "145.motorbikes-101 144\n",
      "146.mountain-bike 145\n",
      "147.mushroom 146\n",
      "148.mussels 147\n",
      "149.necktie 148\n",
      "150.octopus 149\n",
      "151.ostrich 150\n",
      "152.owl 151\n",
      "153.palm-pilot 152\n",
      "154.palm-tree 153\n",
      "155.paperclip 154\n",
      "156.paper-shredder 155\n",
      "157.pci-card 156\n",
      "158.penguin 157\n",
      "159.people 158\n",
      "160.pez-dispenser 159\n",
      "161.photocopier 160\n",
      "162.picnic-table 161\n",
      "163.playing-card 162\n",
      "164.porcupine 163\n",
      "165.pram 164\n",
      "166.praying-mantis 165\n",
      "167.pyramid 166\n",
      "168.raccoon 167\n",
      "169.radio-telescope 168\n",
      "170.rainbow 169\n",
      "171.refrigerator 170\n",
      "172.revolver-101 171\n",
      "173.rifle 172\n",
      "174.rotary-phone 173\n",
      "175.roulette-wheel 174\n",
      "176.saddle 175\n",
      "177.saturn 176\n",
      "178.school-bus 177\n",
      "179.scorpion-101 178\n",
      "180.screwdriver 179\n",
      "181.segway 180\n",
      "182.self-propelled-lawn-mower 181\n",
      "183.sextant 182\n",
      "184.sheet-music 183\n",
      "185.skateboard 184\n",
      "186.skunk 185\n",
      "187.skyscraper 186\n",
      "188.smokestack 187\n",
      "189.snail 188\n",
      "190.snake 189\n",
      "191.sneaker 190\n",
      "192.snowmobile 191\n",
      "193.soccer-ball 192\n",
      "194.socks 193\n",
      "195.soda-can 194\n",
      "196.spaghetti 195\n",
      "197.speed-boat 196\n",
      "198.spider 197\n",
      "199.spoon 198\n",
      "200.stained-glass 199\n",
      "201.starfish-101 200\n",
      "202.steering-wheel 201\n",
      "203.stirrups 202\n",
      "204.sunflower-101 203\n",
      "205.superman 204\n",
      "206.sushi 205\n",
      "207.swan 206\n",
      "208.swiss-army-knife 207\n",
      "209.sword 208\n",
      "210.syringe 209\n",
      "211.tambourine 210\n",
      "212.teapot 211\n",
      "213.teddy-bear 212\n",
      "214.teepee 213\n",
      "215.telephone-box 214\n",
      "216.tennis-ball 215\n",
      "217.tennis-court 216\n",
      "218.tennis-racket 217\n",
      "219.theodolite 218\n",
      "220.toaster 219\n",
      "221.tomato 220\n",
      "222.tombstone 221\n",
      "223.top-hat 222\n",
      "224.touring-bike 223\n",
      "225.tower-pisa 224\n",
      "226.traffic-light 225\n",
      "227.treadmill 226\n",
      "228.triceratops 227\n",
      "229.tricycle 228\n",
      "230.trilobite-101 229\n",
      "231.tripod 230\n",
      "232.t-shirt 231\n",
      "233.tuning-fork 232\n",
      "234.tweezer 233\n",
      "235.umbrella-101 234\n",
      "236.unicorn 235\n",
      "237.vcr 236\n",
      "238.video-projector 237\n",
      "239.washing-machine 238\n",
      "240.watch-101 239\n",
      "241.waterfall 240\n",
      "242.watermelon 241\n",
      "243.welding-mask 242\n",
      "244.wheelbarrow 243\n",
      "245.windmill 244\n",
      "246.wine-bottle 245\n",
      "247.xylophone 246\n",
      "248.yarmulke 247\n",
      "249.yo-yo 248\n",
      "250.zebra 249\n",
      "251.airplanes-101 250\n",
      "252.car-side-101 251\n",
      "253.faces-easy-101 252\n",
      "254.greyhound 253\n",
      "255.tennis-shoes 254\n",
      "256.toad 255\n",
      "257.clutter 256\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p caltech_256_train_60\n",
    "for i in 256_ObjectCategories/*; do\n",
    "    c=`basename $i`\n",
    "    mkdir -p caltech_256_train_60/$c\n",
    "    for j in `ls $i/*.jpg | shuf | head -n 60`; do\n",
    "        mv $j caltech_256_train_60/$c/\n",
    "    done\n",
    "done\n",
    "\n",
    "python im2rec.py --list --recursive --resize 224 --num-thread 16 caltech-256-60-train caltech_256_train_60/\n",
    "python im2rec.py --list --recursive --resize 224 --num-thread 16 caltech-256-60-val 256_ObjectCategories/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A .lst file is a tab-separated file with three columns that contains a list of image files. The first column specifies the image index, the second column specifies the class label index for the image, and the third column specifies the relative path of the image file. The image index in the first column should be unique across all of the images. Here we make an image list file using the im2rec tool from MXNet. You can also create the .lst file in your own way. An example of .lst file is shown as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14204\t236.000000\t237.vcr/237_0069.jpg\n",
      "14861\t247.000000\t248.yarmulke/248_0058.jpg\n",
      "10123\t168.000000\t169.radio-telescope/169_0064.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 ./caltech-256-60-train.lst > example.lst\n",
    "f = open('example.lst','r')\n",
    "lst_content = f.read()\n",
    "print(lst_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the data stored in S3 bucket. The image and lst files will be converted to RecordIO file internelly by the image classification algorithm. But if you wantrf to do the conversion, the following cell shows you how you would do it using the im2rec tool. Note that this is just an example of creating RecordIO files. We are not using them for training in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python im2rec.py --num-thread 16 caltech-256-60-val 256_ObjectCategories/\n",
    "python im2rec.py --num-thread 16 caltech-256-60-train caltech_256_train_60/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are bringing your own image files to train, please ensure that the .lst file follows the same format as described above. In order to train with the lst format interface, passing the lst file for both training and validation in the appropriate format is mandatory. Once we have the data available in the correct format for training, the next step is to upload the image and .lst file to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four channels: train, validation, train_lst, and validation_lst\n",
    "s3train = 's3://{}/image-classification/train/'.format(bucket)\n",
    "s3validation = 's3://{}/image-classification/validation/'.format(bucket)\n",
    "\n",
    "#Uplload Rec files\n",
    "!aws s3 cp caltech-256-60-train.rec $s3train --quiet\n",
    "!aws s3 cp caltech-256-60-val.rec $s3validation --quiet\n",
    "\n",
    "# upload the image files to train and validation channels\n",
    "#!aws s3 cp caltech_256_train_60 $s3train --recursive --quiet\n",
    "#!aws s3 cp 256_ObjectCategories $s3validation --recursive --quiet\n",
    "\n",
    "# upload the lst files to train_lst and validation_lst channels\n",
    "#!aws s3 cp caltech-256-60-train.lst $s3train_lst --quiet\n",
    "#!aws s3 cp caltech-256-60-val.lst $s3validation_lst --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ResNet model\n",
    "\n",
    "\n",
    "\n",
    "## Training parameters\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include:\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html\n",
    "\n",
    "* **num_layers**: The number of layers (depth) for the network. Valid values: For data with large image size (224x224) positive integer in [18, 34, 50, 101, 152, 200] or for data with small image size (28x28) use [20, 32, 44, 56, 110]\n",
    "* **image_shape**: The input image dimensions,'num_channels, height, width', for the network. It should be no larger than the actual image size. The number of channels should be same as the actual image.\n",
    "* **num_classes**: This is the number of output classes for the new dataset. Imagenet was trained with 1000 output classes but the number of output classes can be changed for fine-tuning. For caltech, we use 257 because it has 256 object categories + 1 clutter class.\n",
    "* **num_training_samples**: This is the total number of training samples. It is set to 15240 for caltech dataset with the current split.\n",
    "* **mini_batch_size**: The number of training samples used for each mini batch. In distributed training, the number of training samples used per batch will be N * mini_batch_size where N is the number of hosts on which training is run.\n",
    "* **epochs**: Number of training epochs.\n",
    "* **learning_rate**: Learning rate for training.\n",
    "* **top_k**: Report the top-k accuracy during training.\n",
    "* **precision_dtype**: Training datatype precision (default: float32). If set to 'float16', the training will be done in mixed_precision mode and will be faster than float32 mode\n",
    "* **use_pretrained_model**:  Flag to use pre-trained model for training. If set to 1, then the pretrained model with the corresponding number of layers is loaded and used for training. Only the top FC layer are reinitialized with random weights. Otherwise, the network is trained from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job.\n",
    "### Training parameters\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include:\n",
    "\n",
    "* **Training instance count**: This is the number of instances on which to run the training. When the number of instances is greater than one, then the image classification algorithm will run in distributed settings. ml.p3.8xlarge has 4 x Nvidia V100.  https://aws.amazon.com/sagemaker/pricing/instance-types/\n",
    "* **Training instance type**: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training \n",
    "* **Output path**: This the s3 folder in which the training output is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "num_gpu = 4\n",
    "\n",
    "ic1 = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p3.8xlarge', \n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic1.set_hyperparameters(num_layers=101,\n",
    "                             image_shape = \"3,224,224\",\n",
    "                             num_classes=257,\n",
    "                             num_training_samples=15420,\n",
    "                             mini_batch_size=64 * num_gpu,\n",
    "                             epochs=2,\n",
    "                             optimizer='adam',\n",
    "                             momentum='.9',\n",
    "                             resize = 224,\n",
    "                             learning_rate=0.0002,\n",
    "                             top_k=5,\n",
    "                             kv_store='dist_sync',\n",
    "                             early_stopping=False,\n",
    "                             precision_dtype='float32',\n",
    "                             use_pretrained_model=1)\n",
    "\n",
    "\n",
    "# For Adam Optmizer specific hyperparameters\n",
    "# beta_1 Default value: 0.9\n",
    "# beta_2 Default value: 0.999\n",
    "# eps    Default value:  1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data specification\n",
    "Set the data type and channels used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our data is in RecordIO format, we use these next 3 lines:\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated', \n",
    "                        content_type='application/x-recordio', s3_data_type='S3Prefix')\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated', \n",
    "                             content_type='application/x-recordio', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "\n",
    "\n",
    "## If we were using image files and lst format, we would use the following\n",
    "\n",
    "#train_lst = sagemaker.session.s3_input(s3train_lst, distribution='ShardedByS3Key',\n",
    "#                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "\n",
    "#validation_lst = sagemaker.session.s3_input(s3validation_lst, distribution='FullyReplicated',\n",
    "#                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "\n",
    "#data_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_lst, \n",
    "#                         'validation_lst': validation_lst}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Start training by calling the fit method of the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-06 14:06:25 Starting - Starting the training job...\n",
      "2020-05-06 14:06:27 Starting - Launching requested ML instances...\n",
      "2020-05-06 14:07:25 Starting - Preparing the instances for training.........\n",
      "2020-05-06 14:08:37 Downloading - Downloading input data........................\n",
      "2020-05-06 14:12:49 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.0002', u'top_k': u'5', u'optimizer': u'adam', u'image_shape': u'3,224,224', u'num_layers': u'101', u'kv_store': u'dist_sync', u'epochs': u'2', u'resize': u'224', u'early_stopping': u'False', u'precision_dtype': u'float32', u'num_classes': u'257', u'mini_batch_size': u'256', u'use_pretrained_model': u'1', u'momentum': u'.9', u'num_training_samples': u'15420'}\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] Final configuration: {u'top_k': u'5', u'optimizer': u'adam', u'learning_rate': u'0.0002', u'kv_store': u'dist_sync', u'epochs': u'2', u'lr_scheduler_factor': 0.1, u'num_layers': u'101', u'num_classes': u'257', u'precision_dtype': u'float32', u'mini_batch_size': u'256', u'early_stopping': u'False', u'resize': u'224', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': u'.9', u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'15420'}\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] multi_label: 0\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] num_layers: 101\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] epochs: 2\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] image resize size: 224\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] optimizer: adam\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] beta_1: 0.9\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] beta_2: 0.999\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] eps: 1e-08\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] learning_rate: 0.0002\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] num_training_samples: 15420\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] mini_batch_size: 256\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] num_classes: 257\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] kv_store: device\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] top_k: 5\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:06 INFO 140249139259200] --------------------\u001b[0m\n",
      "\u001b[34m[14:13:06] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[14:13:06] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:07 INFO 140249139259200] Setting number of threads: 31\u001b[0m\n",
      "\n",
      "2020-05-06 14:13:02 Training - Training image download completed. Training in progress.\u001b[34m[14:13:24] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:34 INFO 140249139259200] Epoch[0] Batch [20]#011Speed: 488.644 samples/sec#011accuracy=0.270275#011top_k_accuracy_5=0.429688\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:40 INFO 140249139259200] Epoch[0] Batch [40]#011Speed: 606.290 samples/sec#011accuracy=0.447599#011top_k_accuracy_5=0.629097\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:46 INFO 140249139259200] Epoch[0] Train-accuracy=0.539909\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:46 INFO 140249139259200] Epoch[0] Train-top_k_accuracy_5=0.716992\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:46 INFO 140249139259200] Epoch[0] Time cost=22.977\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:53 INFO 140249139259200] Epoch[0] Validation-accuracy=0.845898\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:54 INFO 140249139259200] Storing the best model with validation accuracy: 0.845898\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:13:54 INFO 140249139259200] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:14:01 INFO 140249139259200] Epoch[1] Batch [20]#011Speed: 759.408 samples/sec#011accuracy=0.846726#011top_k_accuracy_5=0.966890\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:14:07 INFO 140249139259200] Epoch[1] Batch [40]#011Speed: 777.707 samples/sec#011accuracy=0.877001#011top_k_accuracy_5=0.973037\u001b[0m\n",
      "\n",
      "2020-05-06 14:14:23 Uploading - Uploading generated training model\u001b[34m[05/06/2020 14:14:13 INFO 140249139259200] Epoch[1] Train-accuracy=0.895703\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:14:13 INFO 140249139259200] Epoch[1] Train-top_k_accuracy_5=0.977214\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:14:13 INFO 140249139259200] Epoch[1] Time cost=19.285\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:14:20 INFO 140249139259200] Epoch[1] Validation-accuracy=0.902278\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:14:20 INFO 140249139259200] Storing the best model with validation accuracy: 0.902278\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:14:21 INFO 140249139259200] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\n",
      "2020-05-06 14:14:50 Completed - Training job completed\n",
      "Training seconds: 373\n",
      "Billable seconds: 373\n",
      "CPU times: user 811 ms, sys: 37.1 ms, total: 848 ms\n",
      "Wall time: 8min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ic1.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managed Spot Training with MXNet\n",
    "\n",
    "For Managed Spot Training using MXNet we need to configure three things:\n",
    "1. Enable the `train_use_spot_instances` constructor arg - a simple self-explanatory boolean.\n",
    "2. Set the `train_max_wait` constructor arg - this is an int arg representing the amount of time you are willing to wait for Spot infrastructure to become available. Some instance types are harder to get at Spot prices and you may have to wait longer. You are not charged for time spent waiting for Spot infrastructure to become available, you're only charged for actual compute time spent once Spot instances have been successfully procured.\n",
    "3. Setup a `checkpoint_s3_uri` constructor arg. This arg will tell SageMaker an S3 location where to save checkpoints (assuming your algorithm has been modified to save checkpoints periodically). While not strictly necessary checkpointing is highly recommended for Manage Spot Training jobs due to the fact that Spot instances can be interrupted with short notice and using checkpoints to resume from the last interruption ensures you don't lose any progress made before the interruption.\n",
    "\n",
    "Feel free to toggle the `train_use_spot_instances` variable to see the effect of running the same job using regular (a.k.a. \"On Demand\") infrastructure.\n",
    "\n",
    "Note that `train_max_wait` can be set if and only if `train_use_spot_instances` is enabled and **must** be greater than or equal to `train_max_run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for running Managed Spot Training\n",
    "\n",
    "train_use_spot_instances = True\n",
    "train_max_wait = 3600\n",
    "import uuid\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_uri = 's3://{}/artifacts/mxnet-checkpoint-{}/'.format(bucket, checkpoint_suffix) if train_use_spot_instances else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic1spot = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p3.8xlarge', \n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 3600,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess,\n",
    "                                         train_use_spot_instances=train_use_spot_instances,\n",
    "                                         train_max_wait=train_max_wait,\n",
    "                                         checkpoint_s3_uri=checkpoint_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpu = 4\n",
    "\n",
    "ic1spot.set_hyperparameters(num_layers=101,\n",
    "                             image_shape = \"3,224,224\",\n",
    "                             num_classes=257,\n",
    "                             num_training_samples=15420,\n",
    "                             mini_batch_size=64 * num_gpu,\n",
    "                             epochs=20,\n",
    "                             optimizer='adam',\n",
    "                             momentum='.9',\n",
    "                             resize = 224,\n",
    "                             learning_rate=0.0002,\n",
    "                             top_k=5,\n",
    "                             kv_store='dist_sync',\n",
    "                             early_stopping=False,\n",
    "                             precision_dtype='float32',\n",
    "                             use_pretrained_model=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-06 14:50:40 Starting - Starting the training job...\n",
      "2020-05-06 14:50:42 Starting - Launching requested ML instances...\n",
      "2020-05-06 14:51:39 Starting - Preparing the instances for training.........\n",
      "2020-05-06 14:52:46 Downloading - Downloading input data...........................\n",
      "2020-05-06 14:57:28 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.0002', u'top_k': u'5', u'optimizer': u'adam', u'image_shape': u'3,224,224', u'num_layers': u'101', u'kv_store': u'dist_sync', u'epochs': u'20', u'resize': u'224', u'early_stopping': u'False', u'precision_dtype': u'float32', u'num_classes': u'257', u'mini_batch_size': u'512', u'use_pretrained_model': u'1', u'momentum': u'.9', u'num_training_samples': u'15420'}\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] Final configuration: {u'top_k': u'5', u'optimizer': u'adam', u'learning_rate': u'0.0002', u'kv_store': u'dist_sync', u'epochs': u'20', u'lr_scheduler_factor': 0.1, u'num_layers': u'101', u'num_classes': u'257', u'precision_dtype': u'float32', u'mini_batch_size': u'512', u'early_stopping': u'False', u'resize': u'224', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': u'.9', u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'15420'}\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] Checkpoint loading and saving are enabled.\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] Checkpoint directories created successfully.\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] No checkpoint hyperparameters found.\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] Saving checkpoint hyperparameters\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] multi_label: 0\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] num_layers: 101\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] epochs: 20\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] image resize size: 224\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] optimizer: adam\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] beta_1: 0.9\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] beta_2: 0.999\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] eps: 1e-08\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] learning_rate: 0.0002\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] num_training_samples: 15420\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] mini_batch_size: 512\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] num_classes: 257\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] kv_store: device\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] top_k: 5\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:32 INFO 140642713495360] --------------------\u001b[0m\n",
      "\u001b[34m[14:57:32] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[14:57:32] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:33 INFO 140642713495360] Setting number of threads: 31\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:33 INFO 140640922875648] Checkpoint: file '/opt/ml/checkpoints/hyperparameters.json' prepared for uploading.\u001b[0m\n",
      "\u001b[34m[14:57:53] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:56 ERROR 140642713495360] [14:57:56] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/storage/./pooled_storage_manager.h:143: cudaMalloc failed: out of memory\n",
      "\u001b[0m\n",
      "\u001b[34mStack trace returned 10 entries:\u001b[0m\n",
      "\u001b[34m[bt] (0) /opt/amazon/lib/libaialgsdataiter.so(dmlc::StackTrace()+0x3d) [0x7fe9ed144cbd]\u001b[0m\n",
      "\u001b[34m[bt] (1) /opt/amazon/lib/libaialgsdataiter.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x1a) [0x7fe9ed144f5a]\u001b[0m\n",
      "\u001b[34m[bt] (2) /opt/amazon/lib/libmxnet.so(mxnet::storage::GPUPooledStorageManager::Alloc(mxnet::Storage::Handle*)+0x1b9) [0x7fe9d6141649]\u001b[0m\n",
      "\u001b[34m[bt] (3) /opt/amazon/lib/libmxnet.so(mxnet::StorageImpl::Alloc(mxnet::Storage::Handle*)+0x4c) [0x7fe9d614399c]\u001b[0m\n",
      "\u001b[34m[bt] (4) /opt/amazon/lib/libaialgsdataiter.so(mxnet::NDArray::NDArray(nnvm::TShape const&, mxnet::Context, bool, int)+0x59e) [0x7fe9ed15f64e]\u001b[0m\n",
      "\u001b[34m[bt] (5) /opt/amazon/lib/libmxnet.so(mxnet::kvstore::CommDevice::Reduce(int, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, int)+0x6df) [0x7fe9d4d6a06f]\u001b[0m\n",
      "\u001b[34m[bt] (6) /opt/amazon/lib/libmxnet.so(mxnet::kvstore::KVStoreLocal::PushImpl(std::vector<int, std::allocator<int> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, int)+0x183) [0x7fe9d4d702f3]\u001b[0m\n",
      "\u001b[34m[bt] (7) /opt/amazon/lib/libmxnet.so(mxnet::kvstore::KVStoreLocal::Push(std::vector<std::string, std::allocator<std::string> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, int)+0x7f) [0x7fe9d4d2d10f]\u001b[0m\n",
      "\u001b[34m[bt] (8) /opt/amazon/lib/libmxnet.so(MXKVStorePushEx+0xd1) [0x7fe9d4b67e01]\u001b[0m\n",
      "\u001b[34m[bt] (9) /opt/amazon/python2.7/lib/python2.7/lib-dynload/_ctypes.so(ffi_call_unix64+0x4c) [0x7fe9ed7d9958]\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[05/06/2020 14:57:56 ERROR 140642713495360] Customer Error: Out of Memory. Please use a larger instance or reduce batch size.\u001b[0m\n",
      "\n",
      "2020-05-06 14:58:11 Uploading - Uploading generated training model\n",
      "2020-05-06 14:58:11 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job image-classification-2020-05-06-14-50-39-889: Failed. Reason: ClientError: Out of Memory. Please use a larger instance or reduce batch size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3045\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2636\u001b[0m                 ),\n\u001b[1;32m   2637\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2638\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2639\u001b[0m             )\n\u001b[1;32m   2640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job image-classification-2020-05-06-14-50-39-889: Failed. Reason: ClientError: Out of Memory. Please use a larger instance or reduce batch size."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ic1spot.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Savings\n",
    "Towards the end of the job you should see two lines of output printed:\n",
    "\n",
    "- `Training seconds: X` : This is the actual compute-time your training job spent\n",
    "- `Billable seconds: Y` : This is the time you will be billed for after Spot discounting is applied.\n",
    "\n",
    "If you enabled the `train_use_spot_instances` var then you should see a notable difference between `X` and `Y` signifying the cost savings you will get for having chosen Managed Spot Training. This should be reflected in an additional line:\n",
    "- `Managed Spot Training savings: (1-Y/X)*100 %`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification Hyperameters Tuning\n",
    "\n",
    "Once we've defined our estimator we can specify the hyperparameters we'd like to tune and their possible values. We have three different types of hyperparameters.\n",
    "\n",
    "Categorical parameters need to take one value from a discrete set. We define this by passing the list of possible values to CategoricalParameter(list)\n",
    "Continuous parameters can take any real number value between the minimum and maximum value, defined by ContinuousParameter(min, max)\n",
    "Integer parameters can take any integer value between the minimum and maximum value, defined by IntegerParameter(min, max)\n",
    "Note, if possible, it's almost always best to specify a value as the least restrictive type. For example, tuning thresh as a continuous value between 0.01 and 0.2 is likely to yield a better result than tuning as a categorical parameter with possible values of 0.01, 0.1, 0.15, or 0.2.\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.debugger import Rule, CollectionConfig, rule_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugger Rules for Deep Learning\n",
    "\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanishing_gradient_rule = Rule.sagemaker(rule_configs.vanishing_gradient())\n",
    "\n",
    "ichpo = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p3.2xlarge', \n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 3600,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess,                                    \n",
    "                                      \n",
    "                                         ## Rule Parameter\n",
    "                                         rules = [vanishing_gradient_rule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpu = 4\n",
    "\n",
    "ichpo.set_hyperparameters(num_layers=101,\n",
    "                             image_shape = \"3,224,224\",\n",
    "                             num_classes=257,\n",
    "                             num_training_samples=15420,\n",
    "                             mini_batch_size=64 * num_gpu,\n",
    "                             optimizer='adam',\n",
    "                             momentum='.9',\n",
    "                             resize = 224,\n",
    "                             learning_rate=0.0002,\n",
    "                             epochs=200,\n",
    "                             top_k=5,\n",
    "                             early_stopping=True,\n",
    "                             use_pretrained_model=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'Adam']),\n",
    "                         'learning_rate': ContinuousParameter(0.001, 0.002),\n",
    "                         'momentum': ContinuousParameter(.6, .99)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll specify the objective metric that we'd like to tune and its definition. This includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'Validation-accuracy'\n",
    "metric_definitions = [{'Name': 'Validation-accuracy',\n",
    "                       'Regex': 'Validation-accuracy=([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(ichpo,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ichpo.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy The Model\n",
    "\n",
    "***\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the topic mixture representing a given document. \n",
    "\n",
    "This section involves several steps,\n",
    "\n",
    "1. [Create Model](#CreateModel) - Create model for the training output\n",
    "1. [Host the model for real-time inference with EI](#HostTheModel) - Create an inference with EI and perform real-time inference using EI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "We now create a SageMaker Model from the training output. Using the model we will create an Endpoint Configuration to start an endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic1_classifier = ic1.deploy(initial_instance_count = 2,\n",
    "                          instance_type = 'ml.p3.2xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iceia_classifier = ic1spot.deploy(initial_instance_count = 2,\n",
    "                             instance_type = 'ml.m5.large',\n",
    "                             accelerator_type = 'ml.eia2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O /tmp/test-1.jpg http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/080.frog/080_0001.jpg\n",
    "!wget -O /tmp/test-2.jpg http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/002.american-flag/002_0004.jpg\n",
    "!wget -O /tmp/test-3.jpg http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/037.chess-board/037_0006.jpg\n",
    "!wget -O /tmp/test-4.jpg http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/086.golden-gate-bridge/086_0010.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name1 = '/tmp/test-1.jpg'\n",
    "file_name2 = '/tmp/test-2.jpg'\n",
    "file_name3 = '/tmp/test-3.jpg'\n",
    "file_name4 = '/tmp/test-4.jpg'\n",
    "# test image\n",
    "from IPython.display import Image\n",
    "Image(file_name1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using our Training Model\n",
    "\n",
    "Evaluate the image through the network for inteference. The network outputs class probabilities and typically, one selects the class with the maximum probability as the final class output.\n",
    "\n",
    "**Note:** The output class detected by the network may not be accurate in this example. To limit the time taken and cost of training, we have trained the model only for 5 epochs. If the network is trained for more epochs (say 20), then the output class will be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(file_name1, 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "    \n",
    "ic1_classifier.content_type = 'application/x-image'\n",
    "iceia_classifier.content_type = 'application/x-image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_ic1 = json.loads(ic1_classifier.predict(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_iceia = json.loads(iceia_classifier.predict(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result will output the probabilities for all classes\n",
    "# find the class with maximum probability and print the class index\n",
    "index1 = np.argmax(result_ic1)\n",
    "indexeia = np.argmax(result_iceia)\n",
    "\n",
    "object_categories = ['ak47', 'american-flag', 'backpack', 'baseball-bat', 'baseball-glove', 'basketball-hoop', 'bat', 'bathtub', 'bear', 'beer-mug', 'billiards', 'binoculars', 'birdbath', 'blimp', 'bonsai-101', 'boom-box', 'bowling-ball', 'bowling-pin', 'boxing-glove', 'brain-101', 'breadmaker', 'buddha-101', 'bulldozer', 'butterfly', 'cactus', 'cake', 'calculator', 'camel', 'cannon', 'canoe', 'car-tire', 'cartman', 'cd', 'centipede', 'cereal-box', 'chandelier-101', 'chess-board', 'chimp', 'chopsticks', 'cockroach', 'coffee-mug', 'coffin', 'coin', 'comet', 'computer-keyboard', 'computer-monitor', 'computer-mouse', 'conch', 'cormorant', 'covered-wagon', 'cowboy-hat', 'crab-101', 'desk-globe', 'diamond-ring', 'dice', 'dog', 'dolphin-101', 'doorknob', 'drinking-straw', 'duck', 'dumb-bell', 'eiffel-tower', 'electric-guitar-101', 'elephant-101', 'elk', 'ewer-101', 'eyeglasses', 'fern', 'fighter-jet', 'fire-extinguisher', 'fire-hydrant', 'fire-truck', 'fireworks', 'flashlight', 'floppy-disk', 'football-helmet', 'french-horn', 'fried-egg', 'frisbee', 'frog', 'frying-pan', 'galaxy', 'gas-pump', 'giraffe', 'goat', 'golden-gate-bridge', 'goldfish', 'golf-ball', 'goose', 'gorilla', 'grand-piano-101', 'grapes', 'grasshopper', 'guitar-pick', 'hamburger', 'hammock', 'harmonica', 'harp', 'harpsichord', 'hawksbill-101', 'head-phones', 'helicopter-101', 'hibiscus', 'homer-simpson', 'horse', 'horseshoe-crab', 'hot-air-balloon', 'hot-dog', 'hot-tub', 'hourglass', 'house-fly', 'human-skeleton', 'hummingbird', 'ibis-101', 'ice-cream-cone', 'iguana', 'ipod', 'iris', 'jesus-christ', 'joy-stick', 'kangaroo-101', 'kayak', 'ketch-101', 'killer-whale', 'knife', 'ladder', 'laptop-101', 'lathe', 'leopards-101', 'license-plate', 'lightbulb', 'light-house', 'lightning', 'llama-101', 'mailbox', 'mandolin', 'mars', 'mattress', 'megaphone', 'menorah-101', 'microscope', 'microwave', 'minaret', 'minotaur', 'motorbikes-101', 'mountain-bike', 'mushroom', 'mussels', 'necktie', 'octopus', 'ostrich', 'owl', 'palm-pilot', 'palm-tree', 'paperclip', 'paper-shredder', 'pci-card', 'penguin', 'people', 'pez-dispenser', 'photocopier', 'picnic-table', 'playing-card', 'porcupine', 'pram', 'praying-mantis', 'pyramid', 'raccoon', 'radio-telescope', 'rainbow', 'refrigerator', 'revolver-101', 'rifle', 'rotary-phone', 'roulette-wheel', 'saddle', 'saturn', 'school-bus', 'scorpion-101', 'screwdriver', 'segway', 'self-propelled-lawn-mower', 'sextant', 'sheet-music', 'skateboard', 'skunk', 'skyscraper', 'smokestack', 'snail', 'snake', 'sneaker', 'snowmobile', 'soccer-ball', 'socks', 'soda-can', 'spaghetti', 'speed-boat', 'spider', 'spoon', 'stained-glass', 'starfish-101', 'steering-wheel', 'stirrups', 'sunflower-101', 'superman', 'sushi', 'swan', 'swiss-army-knife', 'sword', 'syringe', 'tambourine', 'teapot', 'teddy-bear', 'teepee', 'telephone-box', 'tennis-ball', 'tennis-court', 'tennis-racket', 'theodolite', 'toaster', 'tomato', 'tombstone', 'top-hat', 'touring-bike', 'tower-pisa', 'traffic-light', 'treadmill', 'triceratops', 'tricycle', 'trilobite-101', 'tripod', 't-shirt', 'tuning-fork', 'tweezer', 'umbrella-101', 'unicorn', 'vcr', 'video-projector', 'washing-machine', 'watch-101', 'waterfall', 'watermelon', 'welding-mask', 'wheelbarrow', 'windmill', 'wine-bottle', 'xylophone', 'yarmulke', 'yo-yo', 'zebra', 'airplanes-101', 'car-side-101', 'faces-easy-101', 'greyhound', 'tennis-shoes', 'toad', 'clutter']\n",
    "\n",
    "print(\"Result: label - \" + object_categories[index1] + \", probability - \" + str(result_ic1[index1]))\n",
    "print(\"Result: label - \" + object_categories[indexeia] + \", probability - \" + str(result_iceia[indexeia]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
